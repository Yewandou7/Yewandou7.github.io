<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PS chapter 1</title>
    <url>/2021/11/27/PS%20chapter%201/</url>
    <content><![CDATA[<script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="1-Introduce-to-Probability"><a href="#1-Introduce-to-Probability" class="headerlink" title="1. Introduce to Probability"></a>1. Introduce to Probability</h3><h4 id="1-1-Events-and-Sample-Spaces"><a href="#1-1-Events-and-Sample-Spaces" class="headerlink" title="1.1 Events and Sample Spaces."></a>1.1 Events and Sample Spaces.</h4><h6 id="Definition-1-1"><a href="#Definition-1-1" class="headerlink" title="Definition 1.1."></a>Definition 1.1.</h6><p>An <strong>experiment</strong> or <strong>trial</strong> is any procedure, real or hypothetical, that can be infinity repeated and has a well-defined set of possible outcomes, known as the sample space. An <strong>event</strong> is a well-defined set of possible outcomes of the experiment.</p>
<h6 id="Definition-1-2"><a href="#Definition-1-2" class="headerlink" title="Definition 1.2."></a>Definition 1.2.</h6><p>The collection of all possible outcomes of an experiment is called the <strong>sample space</strong> of the experiment.</p>
<p>​    If we think the sample space as a set, the outcomes in the set and events are subsets of the sample space.</p>
<h6 id="Theorem-1-1"><a href="#Theorem-1-1" class="headerlink" title="Theorem 1.1."></a>Theorem 1.1.</h6><p>Let $A, B, C$ be sets. If $A \subset B$ and $B\subset A$, then $A=B$. If $A\subset B$ and $B\subset C$, then $A \subset C$.</p>
<h6 id="Definition-1-3"><a href="#Definition-1-3" class="headerlink" title="Definition 1.3."></a>Definition 1.3.</h6><p>The set that contains no elements is called <strong>empty set</strong> denote by $\varnothing$. </p>
<h6 id="Theorem-1-2"><a href="#Theorem-1-2" class="headerlink" title="Theorem 1.2."></a>Theorem 1.2.</h6><p>If $A$ is a set, then $\varnothing \subset A$.</p>
<h6 id="Definition-1-4"><a href="#Definition-1-4" class="headerlink" title="Definition 1.4."></a>Definition 1.4.</h6><p>A set that contains only finitely many elements are called a <strong>finite set</strong>. Otherwise, it is called an <strong>infinite set</strong>. An infinite set $A$ is called <strong>countable</strong> if there is a one-to-one map between $A$ and $\mathbb{N}$. A set is called <strong>uncountable</strong> if it is neither finite nor countable.</p>
<p>​    <a href="https://en.wikipedia.org/wiki/Cardinal_number">More</a> about cardinal number. We don’t talk about it in Probability and Statistics.</p>
<h6 id="Definition-1-5"><a href="#Definition-1-5" class="headerlink" title="Definition 1.5."></a>Definition 1.5.</h6><p>The <strong>complement</strong> of $A$ is the set of all elements that do not belong to $A$, denoted by $A^c$.</p>
<h6 id="Theorem-1-3"><a href="#Theorem-1-3" class="headerlink" title="Theorem 1.3."></a>Theorem 1.3.</h6><p>Let $A$ be a set, then $(A^c)^c = A$. Let $X$ be the whole set, then $X^c = \varnothing$ and $\varnothing^c = X$. </p>
<h6 id="Definition-1-6"><a href="#Definition-1-6" class="headerlink" title="Definition 1.6."></a>Definition 1.6.</h6><p>Let $A, B$ be two sets. The <strong>union</strong> of $A$ and $B$, denoted by $A\cup B$ is defined as </p>
<script type="math/tex; mode=display">
A\cup B = \{x:x\in A\ \text{or}\ x\in B\}</script><p>The <strong>intersection</strong> of $A$ and $B$, denoted by $A \cap B$ is defined as </p>
<script type="math/tex; mode=display">
A\cap B = \{x:x\in A\ \text{and}\ x\in B\}</script><h6 id="Conditions-of-Sample-Spaces-Not-every-set-of-possible-outcomes-will-be-called-an-event"><a href="#Conditions-of-Sample-Spaces-Not-every-set-of-possible-outcomes-will-be-called-an-event" class="headerlink" title="Conditions of Sample Spaces (Not every set of possible outcomes will be called an event)"></a>Conditions of Sample Spaces (Not every set of possible outcomes will be called an event)</h6><ul>
<li>The sample space $\Omega$ must be an event.</li>
<li>If $A \subset \Omega$, then $A^c$ is also an event.</li>
<li>If $A_1,A_2\cdots$ is a countable collection of events, then $\cup_{j=1}^\infty A_j$ is also an event. (In fact, the form of $A_j$ means it must be a countable collection)</li>
</ul>
<h6 id="Definition-1-7"><a href="#Definition-1-7" class="headerlink" title="Definition 1.7."></a>Definition 1.7.</h6><p>Two sets $A$ and $B$ are <strong>disjoint (mutually exclusive)</strong> if $A$ and $B$ have no elements in common, that is if $A\cap B = \varnothing$</p>
<h6 id="Theorem-1-4-De-Morgan’s-Laws"><a href="#Theorem-1-4-De-Morgan’s-Laws" class="headerlink" title="Theorem 1.4 (De Morgan’s Laws)."></a>Theorem 1.4 (De Morgan’s Laws).</h6><p>For every two sets $A$ and $B$ we have </p>
<script type="math/tex; mode=display">
(A\cup B)^c = A^c\cap B^c\hspace{1cm}and\hspace{1cm}(A\cap B)^c = A^c \cup B^c</script><h6 id="Theorem-1-5-Distributive-Properties"><a href="#Theorem-1-5-Distributive-Properties" class="headerlink" title="Theorem 1.5 (Distributive Properties)."></a>Theorem 1.5 (Distributive Properties).</h6><p>For sets $A, B$ and $C$, we have </p>
<script type="math/tex; mode=display">
A\cap(B\cup C) = (A\cap B) \cup (A\cap C)\hspace{1cm}and\hspace{1cm}A\cup(B\cap C) = (A\cup B) \cap (A\cup C)</script><h6 id="Theorem-1-6-Partitioning-a-Set"><a href="#Theorem-1-6-Partitioning-a-Set" class="headerlink" title="Theorem 1.6 (Partitioning a Set)."></a>Theorem 1.6 (Partitioning a Set).</h6><p>For sets $A$ and $B$, we can partition </p>
<script type="math/tex; mode=display">
A = (A\cap B)\cup (A\cap B^c)</script><p>In addition </p>
<script type="math/tex; mode=display">
A \cup B = B \cup (A\cap B^c)</script><hr>
<h4 id="1-2-The-Definition-of-Probability"><a href="#1-2-The-Definition-of-Probability" class="headerlink" title="1.2 The Definition of Probability"></a>1.2 The Definition of Probability</h4><h6 id="Definition-1-8-Axioms-of-Probability"><a href="#Definition-1-8-Axioms-of-Probability" class="headerlink" title="Definition 1.8 (Axioms of Probability)."></a>Definition 1.8 (Axioms of Probability).</h6><p>Let $\Omega$ be a sample space and $A$ be an event. A <strong>probability measure</strong> on $\Omega$ is a map $\mathbf{P}$ from the set all events to the real interval $[0,1]$ that satisfies the following three axioms.</p>
<ol>
<li><p>For every event $A$, <script type="math/tex">\P{A} \geq 0</script>. </p>
</li>
<li><p>For the whole sample space $\Omega$, <script type="math/tex">\P{\Omega}=1</script>. </p>
</li>
<li><p>For every infinite sequence of disjoint events $A_1,A_2\cdots$,</p>
<script type="math/tex; mode=display">
\P{\cup_{j=1}^\infty A_j}=\sum_{j=1}^\infty \P{A_j}</script></li>
</ol>
<h6 id="Theorem-1-7"><a href="#Theorem-1-7" class="headerlink" title="Theorem 1.7."></a>Theorem 1.7.</h6><p>For every finite sequence of $n$ disjoint events  $A_1,A_2\cdots A_n$ </p>
<script type="math/tex; mode=display">
\P{\bigcup_{j=1}^n A_j} = \sum_{j=1}^n \P{A_j}</script><p>$Proof.$ Let $A_j = \varnothing$ for $j&gt;n$, then </p>
<script type="math/tex; mode=display">
\P{\cup_{j=1}^n A_j} = \P{\cup_{j=1}^\infty A_j} = \sum_{j=1}^\infty \P{A_j} = \sum_{j=1}^n \P{A_j}</script><p>Why <script type="math/tex">\P{\varnothing} = 0</script>? Consider a sequence $A_1 = \Omega$ and $A_j = \varnothing$ for $j&gt;1$, which follows </p>
<script type="math/tex; mode=display">
1 = 1+\sum_{j=2}^n \P{\varnothing} \Rightarrow \P{\varnothing} = 0</script><h6 id="Theorem-1-8"><a href="#Theorem-1-8" class="headerlink" title="Theorem 1.8."></a>Theorem 1.8.</h6><p>For every event $A$, <script type="math/tex">\P{A^c} = 1-\P{A}</script>. </p>
<p>$Proof.$ Use Theorem $1.7$ </p>
<h6 id="Theorem-1-9"><a href="#Theorem-1-9" class="headerlink" title="Theorem 1.9."></a>Theorem 1.9.</h6><p>For every event $A$, we have <script type="math/tex">0\leq \P{A}\leq 1</script>.</p>
<p>$Proof.$ Follows the definition of probability and sample space.</p>
<h6 id="Theorem-1-10"><a href="#Theorem-1-10" class="headerlink" title="Theorem 1.10."></a>Theorem 1.10.</h6><p>For events $A$ and $B$, we have </p>
<script type="math/tex; mode=display">
\P{A\cap B^c} = \P{A} -\P{A\cap B}</script><p>$Proof.$ Use partition $A = (A\cap B^c) \cup (A\cap B)$ </p>
<h6 id="Corollary-1"><a href="#Corollary-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>If $A \subset B$, then <script type="math/tex">\P{A} \leq \P{B}</script></p>
<p>$Proof.$ <script type="math/tex">\P{B} = \P{A\cap B^c} + \P{A\cap B} = \P{A}+\P{A\cap B^c} \geq \P{A}</script></p>
<h6 id="Theorem-1-11"><a href="#Theorem-1-11" class="headerlink" title="Theorem 1.11."></a>Theorem 1.11.</h6><p>For events $A$ and $B$, we have <script type="math/tex">\P{A\cup B} = \P{A}+\P{B} - \P{A\cap B}</script></p>
<p>$Proof.$ </p>
<script type="math/tex; mode=display">
\P{A\cup B} = \P{B} + \P{A\cap B^c}\\
\P{A} = \P{A\cap B} + \P{A\cap B^c}</script><h6 id="Corollary-1-1"><a href="#Corollary-1-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>For a sequence $A_1,A_2\cdots, A_n$, we have </p>
<script type="math/tex; mode=display">
\P{\bigcup_{i=1}^n A_i} = \sum_{j=1}^n(-1)^{j-1} \sum_{\substack{I_j\subset\{1,\cdots,n\}\\|I_j|=j}} \P{\bigcap_{i\in I_j} A_i}</script><p>$Proof.$ By induction.</p>
<hr>
<h4 id="1-3-Finite-Sample-Spaces"><a href="#1-3-Finite-Sample-Spaces" class="headerlink" title="1.3. Finite Sample Spaces."></a>1.3. Finite Sample Spaces.</h4><h6 id="Definition-1-9"><a href="#Definition-1-9" class="headerlink" title="Definition 1.9."></a>Definition 1.9.</h6><p>A sample space $\Omega$ containing $n$ outcomes $s_1,\cdots, s_n$ is called a <strong>simple sample space</strong> if the probability assigned to each out come is $\frac1n$</p>
<p>​    If $A$ is an event from a simple sample space and $A$ contains $m$ elements, then <script type="math/tex">\P{A} = \frac{m}{n}</script></p>
<hr>
<h4 id="1-4-Counting-Methods"><a href="#1-4-Counting-Methods" class="headerlink" title="1.4. Counting Methods"></a>1.4. Counting Methods</h4><p>​    This part is easy for those who has already known it. I don’t want to waste too much time summarizing this part. Thus I will skip all the theorems or definitions.</p>
<hr>
<h4 id="1-5-Combinatorial-Methods"><a href="#1-5-Combinatorial-Methods" class="headerlink" title="1.5. Combinatorial Methods"></a>1.5. Combinatorial Methods</h4><p>​    The same as $1.4.$ But I will summarize some combinatorial identities.</p>
<h6 id="Definition-1-11"><a href="#Definition-1-11" class="headerlink" title="Definition 1.11."></a>Definition 1.11.</h6><p>Consider a set with $n$ elements. Each subset of size $k$ chosen from this set is called a <strong>combination of $n$ elements</strong> taken $k$ at a time. We denote the number of distinct such combinations by $C_{n,k}$</p>
<h6 id="Definition-1-12"><a href="#Definition-1-12" class="headerlink" title="Definition 1.12."></a>Definition 1.12.</h6><p>The number $C_{n,k}$ is also denoted by the symbol $n\choose k$. That is, for $k=0,1,\cdots,n$, </p>
<script type="math/tex; mode=display">
{n\choose k} = \frac{n!}{k!(n-k)!}</script><h6 id="Combinatorial-identities-1"><a href="#Combinatorial-identities-1" class="headerlink" title="Combinatorial identities 1"></a>Combinatorial identities 1</h6><script type="math/tex; mode=display">
{n\choose k} = {n\choose n-k}</script><h6 id="Combinatorial-identities-2"><a href="#Combinatorial-identities-2" class="headerlink" title="Combinatorial identities 2"></a>Combinatorial identities 2</h6><script type="math/tex; mode=display">
\sum_{k=0}^n {n \choose k} = 2^n\hspace{2cm}\sum_{k=0}^n(-1)^k{n\choose k} = 0</script><h6 id="Combinatorial-identities-3"><a href="#Combinatorial-identities-3" class="headerlink" title="Combinatorial identities 3"></a>Combinatorial identities 3</h6><script type="math/tex; mode=display">
{n\choose k} + {n\choose k+1} = {n+1\choose k+1}</script><h6 id="Combinatorial-identities-4"><a href="#Combinatorial-identities-4" class="headerlink" title="Combinatorial identities 4"></a>Combinatorial identities 4</h6><script type="math/tex; mode=display">
\sum_{k=0}^m {n+k\choose k} = {n+m+1\choose m}</script><h6 id="Combinatorial-identities-5"><a href="#Combinatorial-identities-5" class="headerlink" title="Combinatorial identities 5"></a>Combinatorial identities 5</h6><script type="math/tex; mode=display">
{n+m\choose k} = \sum_{i=0}^k {n\choose i}\cdot {m\choose k-i}</script><h6 id="Combinatorial-identities-6"><a href="#Combinatorial-identities-6" class="headerlink" title="Combinatorial identities 6"></a>Combinatorial identities 6</h6><script type="math/tex; mode=display">
{2n \choose n} = \sum_{i=0}^n {n\choose i}^2</script><hr>
<h4 id="1-6-Multinomial-Coefficients"><a href="#1-6-Multinomial-Coefficients" class="headerlink" title="1.6. Multinomial Coefficients"></a>1.6. Multinomial Coefficients</h4><h6 id="Definition-1-13"><a href="#Definition-1-13" class="headerlink" title="Definition 1.13."></a>Definition 1.13.</h6><p>A <strong>multinomial coefficient</strong> is </p>
<script type="math/tex; mode=display">
{n\choose n_1,n_2,\cdots,n_k} = \frac{n!}{n_1!n_2!\cdots n_k!}</script><p>where $n_1+n_2+\cdots+n_k = n$ </p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>第一章还不是那么抽象, 讲了一些集合的东西以及event的定义. 比较绕的可能就是event是一个定义出的东西而不是任一possible outcomes的set. 接下来定义probability measure是一个从sample space到[0,1]上的一个映射, 以及给出了simple sample space 的定义以及如何计算其中的概率. 个人认为是在概率论中是很具体也很符合直觉的东西了… </p>
<p>这一章里好像最难的是计数的部分, 但我觉得这部分确实没什么好写的, 多做点题就会了…</p>
]]></content>
      <categories>
        <category>Probability and Statistics</category>
      </categories>
      <tags>
        <tag>Study</tag>
      </tags>
  </entry>
  <entry>
    <title>PS chapter 2</title>
    <url>/2021/11/27/PS-chapter-2/</url>
    <content><![CDATA[<script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="2-Conditional-Probability"><a href="#2-Conditional-Probability" class="headerlink" title="2. Conditional Probability"></a>2. Conditional Probability</h3><h4 id="2-1-The-Definition-of-Condition-Probability"><a href="#2-1-The-Definition-of-Condition-Probability" class="headerlink" title="2.1 The Definition of Condition Probability"></a>2.1 The Definition of Condition Probability</h4><p> A major use of probability in statistical inference is the updating of probability when certain events are observed. The updated probability of event $A$ after we learn that event $B$ has occurred is the conditional probability of $A$ given $B$.</p>
<h6 id="Definition-2-1"><a href="#Definition-2-1" class="headerlink" title="Definition 2.1."></a>Definition 2.1.</h6><p>The <strong>condition probability</strong> of $A$ given $B$ is defined as </p>
<script type="math/tex; mode=display">
\P{A|B} = \frac{\P{A\cap B}}{\P{B}}</script><p> provided <script type="math/tex">\P{B} >0</script>.</p>
<p>​    And the conditional probability <script type="math/tex">\P{A|B}</script> is not defined if <script type="math/tex">\P{B}=0</script>.</p>
<h6 id="Theorem-2-1-Multiplication-Rule"><a href="#Theorem-2-1-Multiplication-Rule" class="headerlink" title="Theorem 2.1 (Multiplication Rule)."></a>Theorem 2.1 (Multiplication Rule).</h6><p>Let $A$, $B$ be events. If <script type="math/tex">\P{B}>0</script>, then </p>
<script type="math/tex; mode=display">
\P{A\cap B} =\P{A|B}\P{B}</script><h6 id="Theorem-2-2"><a href="#Theorem-2-2" class="headerlink" title="Theorem 2.2."></a>Theorem 2.2.</h6><p>Suppose that $A_1,A_2,\cdots,A_n$ are events such that <script type="math/tex">\P{A_1\cap A_2\cap\cdots\cap A_{n-1}} >0</script>. Then </p>
<script type="math/tex; mode=display">
\P{A_1\cap A_2 \cap\cdots\cap A_n} = \P{A_1}\P{A_2|A_1}\cdots\P{A_n|A_1\cap A_2\cap\cdots\cap A_{n-1}}</script><h6 id="Theorem-2-3"><a href="#Theorem-2-3" class="headerlink" title="Theorem 2.3."></a>Theorem 2.3.</h6><p>Suppose that $A_1,A_2,\cdots A_n$, $B$ are events such that <script type="math/tex">\P{B}>0</script> and <script type="math/tex">\P{A_1\cap A_2\cap\cdots \cap A_{n-1}|B}>0</script>. Then </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{A_1\cap A_2 \cap\cdots\cap A_n|B} =& \P{A_1|B}\P{A_2|A_1\cap B} \cdots\\
&\times\P{A_n|A_1\cap A_2\cap\cdots\cap A_{n-1}\cap B}
\end{align*}</script><h6 id="Definition-2-2"><a href="#Definition-2-2" class="headerlink" title="Definition 2.2."></a>Definition 2.2.</h6><p>Let $\Omega$ be a sample space and let $B_1, \cdots B_k$ be $k$ events in $\Omega$. If $B_1,\cdots,B_k$ are disjoint and $\cup_{j=1}^k B_j = \Omega$, then these events form a <strong>partition</strong> of $\Omega$. </p>
<h6 id="Theorem-2-4-Low-of-total-probability"><a href="#Theorem-2-4-Low-of-total-probability" class="headerlink" title="Theorem 2.4 (Low of total probability)."></a>Theorem 2.4 (Low of total probability).</h6><p>Suppose that $B_1,\cdots,B_k$ form a partition of the space $\Omega$ and <script type="math/tex">\P{B_j}>0</script> for all $j$. Then for every event $A \subset \Omega$, </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j}\P{A|B_j}</script><p>$Proof.$ We can rewrite $A$ as </p>
<script type="math/tex; mode=display">
A = (B_1 \cap A) \cup (B_2\cap A) \cup\cdots \cup(B_k \cap A)</script><p>Thus </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j\cap A} = \sum_{j=1}^k \P{B_j}\P{A|B_j}</script><h6 id="Theorem-2-5"><a href="#Theorem-2-5" class="headerlink" title="Theorem 2.5."></a>Theorem 2.5.</h6><p>The law of total probability has an analog conditional on another event $C$ </p>
<script type="math/tex; mode=display">
\P{A|C} = \sum_{j=1}^k \P{B_j|C}\P{A|B_j\cap C}</script><p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{B_j|C}\P{A|B_j\cap C} = \frac{\P{B_j \cap C}\P{A\cap B_j \cap C}}{\P{C} \P{B_j\cap C}} = \P{A\cap B_j|C}</script><p>Thus </p>
<script type="math/tex; mode=display">
\sum_{j=1}^k \P{B_j|C}\P{A|B_j\cap C} = \sum_{j=1}^k \frac{\P{A\cap C\cap B_j}}{\P{C}} = \frac{\P{A\cap C}}{\P{C}} = \P{A|C}</script><h4 id="2-2-Independent-Events"><a href="#2-2-Independent-Events" class="headerlink" title="2.2 Independent Events."></a>2.2 Independent Events.</h4><p>If learning that $B$ has occurred does not change the probability of $A$, then we say that $A$ and $B$ are independent.</p>
<h6 id="Definition-2-3"><a href="#Definition-2-3" class="headerlink" title="Definition 2.3."></a>Definition 2.3.</h6><p>Two events $A$ and $B$ are <strong>independent</strong> if </p>
<script type="math/tex; mode=display">
\P{A\cap B} = \P{A}\P{B}</script><h6 id="Theorem-2-6"><a href="#Theorem-2-6" class="headerlink" title="Theorem 2.6."></a>Theorem 2.6.</h6><p>Suppose <script type="math/tex">\P{B}>0</script> or <script type="math/tex">\P{A}>0</script>. Then $A$ and $B$ are independent if and only if <script type="math/tex">\P{A|B} = \P{A}</script> or <script type="math/tex">\P{B|A} = \P{B}</script>, respectively.</p>
<h6 id="Theorem-2-7"><a href="#Theorem-2-7" class="headerlink" title="Theorem 2.7."></a>Theorem 2.7.</h6><p>If $A$ and $B$ are independent, then $A$ and $B^c$ are also independent.</p>
<p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{A\cap B^c} = \P{A} - \P{A\cap B} = \P{A}(1-\P{B}) = \P{A}\P{B^c}</script><h6 id="Definition-2-4-Mutually-independent"><a href="#Definition-2-4-Mutually-independent" class="headerlink" title="Definition 2.4 (Mutually independent)."></a>Definition 2.4 (Mutually independent).</h6><p>The $k$ events $A_1,\cdots,A_k$ are <strong>mutually independent</strong> if for any subset <script type="math/tex">\mathcal{M} \subset \{1,\cdots,k\}</script>, </p>
<script type="math/tex; mode=display">
\P{\bigcap_{j \in \mathcal{M}}A_j} = \prod_{j \in \mathcal{M}} \P{A_j}</script><p>​    Note: The arbitrariness of $\mathcal{M}$ is very important for mutually independent. We can give a conquer  example to show that although $A_j$ are <strong>pairwise independent</strong>, $A_j$ are not mutually independent.</p>
<h6 id="Theorem-2-8"><a href="#Theorem-2-8" class="headerlink" title="Theorem 2.8."></a>Theorem 2.8.</h6><p>Let $A_i (i=1,\cdots,k)$ be events that <script type="math/tex">\P{\cap_{i=1}^k A_i}>0</script>. Then $A_i$ are independent if and only if for every two joint subsets $\mathcal{I},\mathcal{J}\subset\{1,\cdots,k\}$, $\mathcal{I}\cap\mathcal{J}=\varnothing$, we have </p>
<script type="math/tex; mode=display">
\P{\bigcap_{i\in\mathcal{I}}A_i|\bigcap_{j\in\mathcal{J}}A_j} = \P{\bigcap_{i\in\mathcal{I}} A_i}.</script><p>​    This theorem says that $k$ events are independent if and only if learning that some of the events occur does not change the probability that any combination of the other event occur.</p>
<h6 id="Definition-2-5"><a href="#Definition-2-5" class="headerlink" title="Definition 2.5."></a>Definition 2.5.</h6><p>We say that event $A_i (i=1,\cdots,k)$ are <strong>conditionally independent</strong> given $B$ if for every subcollection $\mathcal{I} \subset\{1,\cdots,k\}$, </p>
<script type="math/tex; mode=display">
\P{\bigcap_{i\in\mathcal{I}} A_i|B} = \prod_{i\in\mathcal{I}} \P{A_i|B}</script><p>​    Note: The definition is identical to that of independent events with modification that all probabilities are now conditional on B. It is important that even if we assume $A_i$ are conditionally independent given $B$, it is <strong>not</strong> necessary that $A_i$ are conditionally independent given $B^c$ </p>
<h6 id="Theorem-2-10"><a href="#Theorem-2-10" class="headerlink" title="Theorem 2.10."></a>Theorem 2.10.</h6><p>Suppose that $A_1, A_2$ and $B$ are events such that <script type="math/tex">\P{A_1\cap B}>0</script>. Then $A_1$ and $A_2$ are conditionally independent given $B$ if and only if <script type="math/tex">\P{A_2|A_1\cap B} = \P{A_2|B}</script></p>
<p>$Proof.$ If $A_1$ and $A_2$ are independent </p>
<script type="math/tex; mode=display">
\P{A_1\cap A_2|B} = \P{A_1|B} \P{A_2|B}</script><p>consider that  <script type="math/tex">\P{A_1\cap A_2 |B} = \P{A_1\cap B}\P{A_2|A_1\cap B}</script>. Comparing could proves the theorem.</p>
<h4 id="2-3-Bayes’-Theorem"><a href="#2-3-Bayes’-Theorem" class="headerlink" title="2.3. Bayes’ Theorem"></a>2.3. Bayes’ Theorem</h4><h6 id="Theorem-2-11-Bayes’-Theorem"><a href="#Theorem-2-11-Bayes’-Theorem" class="headerlink" title="Theorem 2.11 (Bayes’ Theorem)."></a>Theorem 2.11 (Bayes’ Theorem).</h6><p>Let $B_i$ for $i=1,\cdots,k$ form a partition of the space $\Omega$ and <script type="math/tex">\P{B_i}>0</script> for all $i$. Let $A$ be an event such that <script type="math/tex">\P{A}>0</script>, then for all $i$:</p>
<script type="math/tex; mode=display">
\P{B_i|A} = \frac{\P{B_i}\P{A|B_i}}{\sum_{j=1}^k\P{B_j}\P{A|B_j}}</script><p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{B_i|A} = \frac{\P{B_i \cap A}}{\P{A}} = \frac{\P{B_i}\P{A|B_i}}{\P{A}}</script><p>and </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j} \P{A|B_j}</script><p>Which proves the theorem.</p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>这一章围绕着条件概率展开, 主要研究的是如果某些events被观测到后, 其余event的概率会发生怎样的变化. 接下来就按照是否会发生影响来定义了事件的相互独立, 以及类似地 mutually independent 和 conditionally independent (这两个是对一个序列的事件而言的). 以及给出了一个很常用的定理Bayes’ Theorem, 这个定理给出了一个<strong>prior probability</strong>与<strong>posterior probability</strong>的关系, 因此也会带来很多反直觉的结果. 应用这个定理时最重要的时要确定每一步的过程的条件究竟是什么样的, 否则必然带来一些奇怪的错误. </p>
]]></content>
      <categories>
        <category>Probability and Statistics</category>
      </categories>
      <tags>
        <tag>Study</tag>
      </tags>
  </entry>
  <entry>
    <title>What&#39;s this?</title>
    <url>/2021/11/25/What&#39;%20this/</url>
    <content><![CDATA[<h3 id="What’s-this"><a href="#What’s-this" class="headerlink" title="What’s this?"></a>What’s this?</h3><p>一个无聊的憨憨整的博客, 内容大概会是很多的学习经历, 要是有出息的话会放一些自己觉得并不trival的东西上来. 前期内容大概就是一些学习笔记之类的, 由于作者学校CS专业课/大部分数学课都是in English的, 很多东西大概也都是用英语写了.</p>
<p>美观在做了别骂了呜呜呜… 这周一定搞成能看的东西.</p>
<hr>
<h3 id="Who-is-Yewandou"><a href="#Who-is-Yewandou" class="headerlink" title="Who is Yewandou?"></a>Who is Yewandou?</h3><p>大概是作者的网名叭, 不是名人, 高中整过一些物理什么的, 现某校CS大二学生. </p>
<h4 id="主要学习方向："><a href="#主要学习方向：" class="headerlink" title="主要学习方向："></a>主要学习方向：</h4><p>无, 主打一个摸鱼</p>
<h4 id="主要获奖经历："><a href="#主要获奖经历：" class="headerlink" title="主要获奖经历："></a>主要获奖经历：</h4><p>躺了一块ICPC region金牌, 一块ICPC region银牌, 无了.</p>
<h4 id="努力方向："><a href="#努力方向：" class="headerlink" title="努力方向："></a>努力方向：</h4><ol>
<li><p>从变态概率论老师手中活下来 </p>
</li>
<li><p>把手上的ML的入门书看完 </p>
</li>
<li><p>把暑假里看的CS172整理一次笔记.</p>
</li>
<li><p>S12上铂金 (先整个小目标)</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>nothing</tag>
      </tags>
  </entry>
  <entry>
    <title>PS chapter 3</title>
    <url>/2021/11/27/PS-chapter-3/</url>
    <content><![CDATA[<script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="3-Random-Variables-and-Distributions"><a href="#3-Random-Variables-and-Distributions" class="headerlink" title="3. Random Variables and Distributions"></a>3. Random Variables and Distributions</h3><h4 id="3-1-Discrete-Random-Variables"><a href="#3-1-Discrete-Random-Variables" class="headerlink" title="3.1. Discrete Random Variables."></a>3.1. Discrete Random Variables.</h4><h6 id="Definition-3-1"><a href="#Definition-3-1" class="headerlink" title="Definition 3.1."></a>Definition 3.1.</h6><p>Consider a random experiment with a sample space $\Omega$. A function $X$, which assigns to each element $s \in \Omega$ one and only one number $X(s) = x$, is called a <strong>random variable</strong>. The <strong>space</strong> or <strong>range</strong> of $X$ is the set of real numbers $\mathcal{D} = \{x:x=X(s), s\in\Omega\}$.</p>
<p>​    In this text, $\mathcal{D}$ generally is a <strong>a countable  set</strong> or <strong>an interval</strong> of real numbers. We call random variables of the first type <strong>discrete</strong> random variables, while we call those of the second type <strong>continuous</strong> random variables.</p>
<p>​    Given a random variable $X$, its range $\mathcal{D}$ becomes the sample space of interest. Besides inducing the sample space $\mathcal{D}$, $X$ also induces a probability which we call the <strong>distribution</strong> of $X$.</p>
<h6 id="Definition-3-2"><a href="#Definition-3-2" class="headerlink" title="Definition 3.2."></a>Definition 3.2.</h6><p>Let $X$ be a discrete random variable that takes values in $\mathcal{D} = \{d_1,\cdots,d_m\}$. Define the function </p>
<script type="math/tex; mode=display">
p_X(d_i) = \P{\{s\in \Omega:X(s) = d_i\}},\ \ \text{for }i=1,\cdots,m</script><p>This is called the <strong>probability mass function (pmf)</strong> of $X$.</p>
<p>​    By the definition, for a discrete random variable to find the probability of an event $A$ from a pmf, we can simply calculate </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{x\in A} p_X(x)</script><h6 id="Theorem-3-1"><a href="#Theorem-3-1" class="headerlink" title="Theorem 3.1."></a>Theorem 3.1.</h6><p>Let $X$ be a discrete random variable with pmf $p_X(x)$. If $x$ is not one the possible values of $X$, then $p_X(x) = 0$. Also, if the sequence $d_1,d_2,\cdots$ includes all the possible values of $X$, then $\sum_{i=1}^\infty p_X(d_i) =1$.<br>$Proof.$ Use the fact that </p>
<script type="math/tex; mode=display">
\sum_{d_i} p_X(d_i) = \sum_{d_i} \P{X=d_i} = \P{\Omega} = 1</script><h4 id="3-2-Bernoulli-Binomial-and-Geometric-Distributions"><a href="#3-2-Bernoulli-Binomial-and-Geometric-Distributions" class="headerlink" title="3.2 Bernoulli, Binomial, and Geometric Distributions."></a>3.2 Bernoulli, Binomial, and Geometric Distributions.</h4><p>We will introduce these distributions in Chapter  5</p>
<h4 id="3-3-Continuous-Distributions"><a href="#3-3-Continuous-Distributions" class="headerlink" title="3.3 Continuous Distributions."></a>3.3 Continuous Distributions.</h4><h6 id="Definition-3-7-Continuous-Distribution"><a href="#Definition-3-7-Continuous-Distribution" class="headerlink" title="Definition 3.7 (Continuous Distribution)."></a>Definition 3.7 (Continuous Distribution).</h6><p>A random variable $X$ is continuous if for some function $f:\mathbb{R} \to \mathbb{R} $ and for any $a\le b\in \mathbb{R}$, </p>
<script type="math/tex; mode=display">
\P{a\le X\le b} = \int_a^b f(x) dx</script><p>The function has to be $f(x) \ge 0$ for all $x$ and $\int_{-\infty}^\infty f(x) dx = 1$. We call $f$ the <strong>probability density function (pdf)</strong> of $X$.</p>
<p>​    Note: it is important that we don’t define $f(x)$ without <script type="math/tex">\P{X}</script>, in other words, the pmf of $X$ is defined by <script type="math/tex">p_X(x) = \P{X=x}</script>, but $f(x) = \frac{dP(X)}{dx}|_x$, we couldn’t find $f(x)$ without find <script type="math/tex">\P{X}</script> firstly. (Right, we have some tricks or skills to solve pdf, but the definition is should follow these steps).</p>
<h4 id="3-4-Cumulative-Distribution-Functions"><a href="#3-4-Cumulative-Distribution-Functions" class="headerlink" title="3.4 Cumulative Distribution Functions."></a>3.4 Cumulative Distribution Functions.</h4><h6 id="Definition-3-9-Cumulative-Distribution-Function"><a href="#Definition-3-9-Cumulative-Distribution-Function" class="headerlink" title="Definition 3.9 (Cumulative Distribution Function)"></a>Definition 3.9 (Cumulative Distribution Function)</h6><p>The <strong>cumulative distribution function (cdf)</strong> $F$ of a random variable $X$ is the function $F:\mathbb{R} \to [0,1]$ defined by </p>
<script type="math/tex; mode=display">
F(a) = \P{X\le a}\ \ \ \text{ for }\ -\infty<x<\infty</script><p>​    Both pmf (pdf) and cdf of a random variable contain all probabilistic information of $X$. In particular, if X is a discrete random variable supported on $\{d_1,\cdots,d_m\}$, its cdf can be obtained by </p>
<script type="math/tex; mode=display">
F(a) = \sum_{d_i\le a} p_X(d_i)</script><h6 id="Proposition-1"><a href="#Proposition-1" class="headerlink" title="Proposition 1."></a>Proposition 1.</h6><p>The cdf $F(x)$ of $X$ satisfies the following three properties.</p>
<ul>
<li><p>For $a\le b$, one has $F(a) \le F(b)$.</p>
</li>
<li><p>Since $F(a)$ is a probability, we have </p>
<script type="math/tex; mode=display">
\lim\limits_{x\to-\infty}F(x) = 0\ \ \ \ \ \lim\limits_{x\to\infty}F(x) = 1</script></li>
<li><p>The cdf <script type="math/tex">F(x) = \P{X\le x}</script>, it is always continuous from the right.</p>
</li>
</ul>
<h6 id="Theorem-3-2"><a href="#Theorem-3-2" class="headerlink" title="Theorem 3.2."></a>Theorem 3.2.</h6><p>Let $X$ be a continuous random variable, and let $f(x)$ and $F(x)$ denote its pdf and cdf, respectively. Then for every real $x$</p>
<script type="math/tex; mode=display">
F(x) = \int_{-\infty}^x f(t)\ dt</script><p>and</p>
<script type="math/tex; mode=display">
\frac{d}{dx} F(x) = f(x)</script><p>at all $x$ where $f(x)$ is continuous.</p>
<h6 id="Proposition-2-More-Properties"><a href="#Proposition-2-More-Properties" class="headerlink" title="Proposition 2 (More Properties)."></a>Proposition 2 (More Properties).</h6><p>For real numbers $x$ and $a&lt;b$, we have</p>
<ul>
<li><script type="math/tex; mode=display">\P{X>x} = 1-F(x)</script></li>
<li><script type="math/tex; mode=display">\P{a<X\le b} = F(b) - F(a)</script></li>
<li><script type="math/tex; mode=display">\P{X<x} = \lim_{t\to x^-} F(t)$$ and $$\P{X=x} = F(x) - \lim_{t\to x^-}F(t)</script></li>
</ul>
<h6 id="Definition-3-10"><a href="#Definition-3-10" class="headerlink" title="Definition 3.10."></a>Definition 3.10.</h6><p>Let $X$ be a random variable and let $p \in(0,1)$. The $p$ <strong>quantile</strong> or <strong>100$p$th percentile</strong> of the distribution of $X$ is the the smallest number $q_p$ s.t.</p>
<script type="math/tex; mode=display">
F(q_p) = \P{X\le q_p} \ge p</script><p>The <strong>median</strong> of a distribution is its $50th$ percentile.</p>
<p>​    I don’t think this is useful…</p>
<h4 id="3-5-Joint-Distribution"><a href="#3-5-Joint-Distribution" class="headerlink" title="3.5. Joint Distribution"></a>3.5. Joint Distribution</h4><h6 id="Definition-3-11-Discrete-Joint-Distributions"><a href="#Definition-3-11-Discrete-Joint-Distributions" class="headerlink" title="Definition 3.11 (Discrete Joint Distributions)."></a>Definition 3.11 (Discrete Joint Distributions).</h6><p> The <strong>joint probability mass function</strong> $p_{X,Y}$ of two discrete random variables $X$ and $Y$ is the function $p_{X,Y}:\mathbb{R}^2\to[0,1]$, define by </p>
<script type="math/tex; mode=display">
p_{X,Y}(a,b) = \P{X=a, Y=b}\hspace{1cm}\text{for}-\infty<a,b<\infty</script><h6 id="Theorem-3-3"><a href="#Theorem-3-3" class="headerlink" title="Theorem 3.3."></a>Theorem 3.3.</h6><p>Let $X$ and $Y$ have a discrete joint distribution. If $(x,y)$ is not one of the possible values of the pair $(X,Y)$, then $p_{X,Y}(x,y)=0$. Moreover, </p>
<script type="math/tex; mode=display">
\sum_{(x,y)\in\mathbb{R}^2}p_{X,Y}(x,y) =1</script><p>For each set $C \subset \mathbb{R}^2$, </p>
<script type="math/tex; mode=display">
\P{(X,Y)\in C} = \sum_{(x,y)\in C} p_{X,Y}(x,y)</script><h6 id="Definition-3-12-Continuous-Joint-Distributions"><a href="#Definition-3-12-Continuous-Joint-Distributions" class="headerlink" title="Definition 3.12 (Continuous Joint Distributions)."></a>Definition 3.12 (Continuous Joint Distributions).</h6><p>Random variables $X$ and $Y$ have a <strong>joint continuous distribution</strong> if for some function $f_{X,Y}:\mathbb{R}^2 \to \mathbb{R}$ and for all numbers $a_1, a_2$ and $b_1,b_2$ with $a_1\le b_1$ and $a_2\le b_2$, </p>
<script type="math/tex; mode=display">
\P{a_1\le X\le b_1, a_2\le Y\le b_2} = \int_{a_1}^{b_1}\int_{a_2}^{b_2} f_{X,Y}(x,y)dxdy</script><p>The function $f_{X,Y}$ has to be nonnegative for all $x$, $y$, and $\int_{-\infty}^\infty \int_{-\infty}^\infty f_{X,Y}(x,y)dxdy = 1$. We call $f_{X,Y}$ the <strong>joint probability density function</strong> of $X$ and $Y$.</p>
<p>​    Note: I will omit the subscript $X,Y$ if there is no ambiguity.</p>
<h6 id="Theorem-3-4"><a href="#Theorem-3-4" class="headerlink" title="Theorem 3.4."></a>Theorem 3.4.</h6><p>For every continuous joint distribution, the following statements hold:</p>
<ul>
<li>Every individual point, and every infinite sequence of points, in the $xy$-plane has probability 0.</li>
<li><p>Let $f$ be one real variable  continuous function defined on $(a,b)$. The sets $\{(x,y):y=f(x), x\in(a,b)\}$ and $\{(x,y):x=f(y), y\in(a,b)\}$ have probability 0.</p>
<p>Note that if $X$ is a continuous random variable and $Y=X$, then <strong>both $X$ and $Y$ are continuous</strong> but $(X,Y)$ lies on a straight line $y=x$ in $\mathbb{R}^2$. Hence, by Theorem 3.4, $X$ and $Y$ <strong>cannot have a continuous joint distribution</strong>.</p>
</li>
</ul>
<p>​    In this part, we always could choose a part $C \subset \mathbb{R}^2$ to make <script type="math/tex">\P{(X,Y)\in C} <0</script>, which could proves this Theorem.</p>
<h6 id="Definition-3-13-Mixed-Joint-Distribution"><a href="#Definition-3-13-Mixed-Joint-Distribution" class="headerlink" title="Definition 3.13 (Mixed Joint Distribution)."></a>Definition 3.13 (Mixed Joint Distribution).</h6><p>Let $X$ be a discrete random variable and $Y$ is continuous, They have a <strong>mixed joint distribution</strong> if there is a function $f$ such that </p>
<script type="math/tex; mode=display">
\P{X\in A\ \text{and }\ Y\in B} = \int_{B}\sum_{x\in A} f(x,y) dy</script><p>for all pair $A, B\subset \mathbb R$ . The function $f(x,y)$ has to be nonnegative and $\int_{-\infty}^\infty \sum_{-\infty}^\infty f(x,y)dy=1$, We call the <strong>mixed joint probability function</strong> of $X$ and $Y$.</p>
<h4 id="3-6-Marginal-Distributions"><a href="#3-6-Marginal-Distributions" class="headerlink" title="3.6. Marginal Distributions"></a>3.6. Marginal Distributions</h4><h6 id="Definition-3-14"><a href="#Definition-3-14" class="headerlink" title="Definition 3.14."></a>Definition 3.14.</h6><p>The <strong>joint cumulative distribution function </strong> of two random variables $X$ and $Y$ is define as </p>
<script type="math/tex; mode=display">
F_{X,Y}(x,y) = \P{X\le x\ \text{and}\ Y\le y}</script><h6 id="Definition-3-15-Marginal-Distribution"><a href="#Definition-3-15-Marginal-Distribution" class="headerlink" title="Definition 3.15 (Marginal Distribution)."></a>Definition 3.15 (Marginal Distribution).</h6><p>Suppose $F_{X,Y}(x,y)$ is the joint cdf for random variables $X$ and $Y$, The <strong>marginal cdf</strong> of $X$ is </p>
<script type="math/tex; mode=display">
F_X(x) = \lim_{y\to\infty} F_{X,Y}(x,y)</script><p>Symmetrically, we can define marginal cdf of $Y$ by taking limit of $F_{X,Y}$ as $x$ goes to infinity.</p>
<p>​    The pmf(pdf) associated with the joint marginal cdf of $X$ is called the <strong>marginal pmf(pdf)</strong> of $X$.</p>
<h6 id="Theorem-3-5"><a href="#Theorem-3-5" class="headerlink" title="Theorem 3.5."></a>Theorem 3.5.</h6><p>Suppose $X$, $Y$ are random variables with a joint pf $f_{X,Y}(x,y)$. Then the marginal pmf (pdf) of $X$ can be obtained by </p>
<script type="math/tex; mode=display">
f_X(x) = \sum_{y=-\infty}^\infty f_{X,Y}(x,y)</script><p>or </p>
<script type="math/tex; mode=display">
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y)\ dy</script><p>depending on whether $Y$ is discrete or continuous.</p>
<h6 id="Definition-3-16-Independent-Random-Variables"><a href="#Definition-3-16-Independent-Random-Variables" class="headerlink" title="Definition 3.16  (Independent Random Variables)"></a>Definition 3.16  (Independent Random Variables)</h6><p>Two random variables $X$ and $Y$ are said to be independent if for all events $A, B\subset \mathbb{R}$, </p>
<script type="math/tex; mode=display">
\P{X\in A\ \text{and}\ Y\in B} = \P{X\in A} \P{Y\in B}.</script><p>Random variables that are <strong>not independent</strong> are called <strong>dependent</strong>.</p>
<p>​    It follows immediately  from the definition that if $X$ and $Y$ are independent, then</p>
<script type="math/tex; mode=display">
\P{X\le x\ \text{and}\ Y\le y} =\P{X\le x}\P{Y\le y}</script><h6 id="Theorem-3-6-Equivalent-Definition-of-Independency"><a href="#Theorem-3-6-Equivalent-Definition-of-Independency" class="headerlink" title="Theorem 3.6 (Equivalent Definition of Independency)."></a>Theorem 3.6 (Equivalent Definition of Independency).</h6><p>Let $F(x,y)$ be the joint cdf of $X$ and $Y$, Let $F_X(x)$ and $F_Y(y)$ be the marginal cdf for $X$ and $Y$ repectively. Then $X$ and $Y$ are independent if and only if </p>
<script type="math/tex; mode=display">
F(a,b) = F_X(a)F_Y(b)</script><p>for all real values $a, b$</p>
<p>​    As a special case we can take $A=\{a\}$ and $B = \{b\}$, then </p>
<script type="math/tex; mode=display">
\P{X=a\ \text{and}\ Y=b} = \P{X=a}\P{Y=b}</script><h6 id="Theorem-3-7"><a href="#Theorem-3-7" class="headerlink" title="Theorem 3.7."></a>Theorem 3.7.</h6><p>Suppose $f(x,y)$ is the joint pmf (pdf) of $X$ and $Y$. Then $X$ and $Y$ are independent if and only if </p>
<script type="math/tex; mode=display">
f(x,y) = f_X(x)f_Y(y)</script><h4 id="3-7-Conditional-Distributions"><a href="#3-7-Conditional-Distributions" class="headerlink" title="3.7. Conditional Distributions"></a>3.7. Conditional Distributions</h4><h6 id="Definition-3-17"><a href="#Definition-3-17" class="headerlink" title="Definition 3.17."></a>Definition 3.17.</h6><p>Let $X$ and $Y$ be random variables with joint pmf (pdf) $f(x,y)$. Let $f_Y$ denote the marginal pmf (pdf) of $Y$ such that for all $f_Y(y)&gt;0$ for all $y$. Define </p>
<script type="math/tex; mode=display">
g_X(x|y) = \frac{f(x,y)}{f_Y(y)}</script><p>Then $g_X(\cdot|y)$ is called the <strong>conditional pmf(pdf)</strong> of $X$ given $Y$. The distribution whose pmf(pdf) is $g_X(\cdot|y)$ is called the <strong>conditional distribution</strong> of $X$ given $Y=y$. </p>
<h4 id="3-8-Multivariate-Distributions"><a href="#3-8-Multivariate-Distributions" class="headerlink" title="3.8. Multivariate Distributions."></a>3.8. Multivariate Distributions.</h4><p>In this section, we shall extend the results for two random variables to an arbitrary finite number $n$ of random variables $X_1,X_2,\cdots,X_n$. In general, the joint distribution of more than two random variables is called a multivariate distribution.</p>
<h6 id="Definition-3-18"><a href="#Definition-3-18" class="headerlink" title="Definition 3.18."></a>Definition 3.18.</h6><p>The <strong>joint cumulative distribution function</strong> of $X_1,\cdots,X_n$, which is defined by </p>
<script type="math/tex; mode=display">
F(a_1,a_2,\cdots,a_n) = \P{X_1\le a_1, \cdots, X_n\le a_n}</script><p>for $-\infty &lt;a_1,\cdots,a_n&lt;\infty$. </p>
<p>If the random variables is discrete, the joint distribution can also be characterized by specifying the joint probability mass function $p$ of $X_1,X_2,\cdots,X_n$, defined by </p>
<script type="math/tex; mode=display">
p(a_1,a_2,\cdots,a_n) = \P{X_1=a_1,\cdots,X_n=a_n}</script><p>for $-\infty &lt; a_1,\cdots,a_n&lt;\infty$ .</p>
<h6 id="Theorem-3-8"><a href="#Theorem-3-8" class="headerlink" title="Theorem 3.8."></a>Theorem 3.8.</h6><p>If $\mathbf{X} = (X_i)_{i=1}^n$ has a joint discrete distribution with pmf $f$, then for every subset $C\subset R^n$, </p>
<script type="math/tex; mode=display">
\P{\bold{X}\in C} = \sum_{\bold{x}\in C}f(\bold{x})</script><h6 id="Definition-3-19"><a href="#Definition-3-19" class="headerlink" title="Definition 3.19."></a>Definition 3.19.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables. The <strong>joint continuous distribution function</strong> is a nonnegative function $f:\mathbb{R}^n \to \mathbb{R}$ such that for all $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ </p>
<script type="math/tex; mode=display">
\P{\cap_{i=1}^n \{a_i\le X_i\le b_i\}} = \int_{a_1}^{b_1}\cdots\int_{a_n}^{b_n} f(x_1,\cdots,x_n)\ dx_1\cdots dx_n</script><p>Here $f$ satisfy $\int f = 1$. We call $f$ the <strong>joint probability density function </strong> of $\mathbf{X}$.</p>
<h6 id="Theorem-3-9"><a href="#Theorem-3-9" class="headerlink" title="Theorem 3.9."></a>Theorem 3.9.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables. The joint pdf $f$ can derived from the joint cdf $F$ by </p>
<script type="math/tex; mode=display">
f(x_1,\cdots,x_n)= \frac{\partial^n}{\partial x_1\cdots\partial x_n}F(x_1,\cdots,x_n)</script><p>​    Similar to the two variable case, the marginal cdf $F_j(x_j)$ of $X_j$ can be obtained by </p>
<script type="math/tex; mode=display">
F_j(x_j) = \P{X_j\le x_j}=\lim\limits_{\substack{x_i\to\infty\\i\neq j}} F(x_1,\cdots,x_n)</script><h6 id="Definition-3-20"><a href="#Definition-3-20" class="headerlink" title="Definition 3.20."></a>Definition 3.20.</h6><p>Random variables $(X_i)_{i=1}^n$ are <strong>independent</strong> if for every $n$ sets $A_1,\cdots,A_n$ of real numbers.</p>
<script type="math/tex; mode=display">
\P{\cap_{i=1}^n\{X_i\subset A_i\}} = \prod_{i=1}^n \P{X_i\subset A_i}.</script><h6 id="Definition-3-21-i-i-d"><a href="#Definition-3-21-i-i-d" class="headerlink" title="Definition 3.21 (i.i.d.)."></a>Definition 3.21 (i.i.d.).</h6><p> A collection of random variables $(X_i)_{i=1}^n$ is called <strong>independent and identically distributed</strong> if each Xi has the same probability distribution and they are mutually independent. This property is often abbreviated as <strong>i.i.d.</strong></p>
<h4 id="3-9-Function-of-One-Random-Variable"><a href="#3-9-Function-of-One-Random-Variable" class="headerlink" title="3.9. Function of One Random Variable."></a>3.9. Function of One Random Variable.</h4><h6 id="Theorem-3-10"><a href="#Theorem-3-10" class="headerlink" title="Theorem 3.10."></a>Theorem 3.10.</h6><p>Let $X$ be a discrete random variable with pmf $p_X(x)$ and let $Y = r(X)$ for some function $r$ defined on the support of $X$. Then the pmf of $Y$ is</p>
<script type="math/tex; mode=display">
p_Y(y) = \P{Y=y} = \P{r(X) = y} =\sum_{r(x)=y} p_X(x)</script><p>​    If $r$ is invertible on the support of $X$, the above theorem can be simplified to </p>
<script type="math/tex; mode=display">
p_Y(y) = \sum_{x=r^{-1}(y)}p_X(x)</script><h6 id="Theorem-3-11-Change-of-Units-Transformation"><a href="#Theorem-3-11-Change-of-Units-Transformation" class="headerlink" title="Theorem 3.11 (Change-of-Units Transformation)."></a>Theorem 3.11 (Change-of-Units Transformation).</h6><p>Suppose $X$ is continuous random variable with pdf $f_X(x)$. If we make a linear variable change $Y=g(X) =aX+b$ for $a\neq 0$, the pdf of $Y$ is </p>
<script type="math/tex; mode=display">
f_Y(y) = \frac1{|a|}f_X(\frac{y-b}a)</script><p>for $y\in\mathbb{R}$.</p>
<p>$Proof.$ By cdf. </p>
<h6 id="Theorem-3-12-Probability-Integral-Transformation"><a href="#Theorem-3-12-Probability-Integral-Transformation" class="headerlink" title="Theorem 3.12 (Probability Integral Transformation)"></a>Theorem 3.12 (Probability Integral Transformation)</h6><p>Let $X$ have a continuous cdf $F$ and let $Y=F(X)$. The distribution of $Y$ is the uniform distribution on the interval $[0,1]$ </p>
<p>$Proof$. It equals to $F(y)=y$ for $0\leq y\leq 1$. Let $q_y$ be the $y$ quantile point of $F$. Since $F$ is continuous, $F(q_y)=y$. Then by the property the quantile function, $X\le q_y$ iff $F(X)\le y$.</p>
<script type="math/tex; mode=display">
F(y) = \P{Y\le y} = \P{X\le q_y} = F(q_y)=y</script><p>Which proves the theorem.</p>
<h6 id="Theorem-3-13"><a href="#Theorem-3-13" class="headerlink" title="Theorem 3.13."></a>Theorem 3.13.</h6><p>too trival</p>
<h4 id="3-10-Functions-of-Two-Random-Variables"><a href="#3-10-Functions-of-Two-Random-Variables" class="headerlink" title="3.10. Functions of Two Random Variables."></a>3.10. Functions of Two Random Variables.</h4><p>Suppose $(X_i)_{i=1}^n$ are random variables with a discrete joint pmf $p_X$. Let </p>
<script type="math/tex; mode=display">
Y_i = r_i(X_1,\cdots,X_n)\hspace{1cm}\text{for }\ 1\le i\le m</script><p><img src="C:\Users\85125\AppData\Roaming\Typora\typora-user-images\image-20211204133102452.png" alt="image-20211204133102452"></p>
<h6 id="Theorem-3-17"><a href="#Theorem-3-17" class="headerlink" title="Theorem 3.17."></a>Theorem 3.17.</h6><p>Let $f_{X_1,X_2}(x_1,x_2)$ be the joint pdf of $X_1$, $X_2$ and let $Y=a_1X_1+a_2X_2+b$ with $a_1\neq 0$, the pdf of $Y$ is </p>
<script type="math/tex; mode=display">
f_Y(y) = \int_{-\infty}^\infty f_{X_1,X_2}(\frac{y-b-a_2x_2}{a_1},x_2)\frac1{|a_1|}dx_2</script><h6 id="Definition-3-22-Convolution"><a href="#Definition-3-22-Convolution" class="headerlink" title="Definition 3.22 (Convolution)."></a>Definition 3.22 (Convolution).</h6><p>Let $X_1, X_2$ be independent continuous random variables and let $Y = X_1 + X_2$. The distribution of Y is called the <strong>convolution</strong> of the distributions of $X_1$ and $X_2$.</p>
<p>​    If $X_1$ and $X_2$ are discrete random variables, then </p>
<script type="math/tex; mode=display">
p_Y(y) =\sum_{x=-\infty}^\infty p_1(x)p_2(y-x)</script><p>​    For continuous random variables $X_1,X_2$ with pdf $f_1,f_2$, then </p>
<script type="math/tex; mode=display">
f(y) = \int_{-\infty}^\infty f_1(x)f_2(y-x)\ dx</script><h6 id="More-transform"><a href="#More-transform" class="headerlink" title="More transform"></a>More transform</h6><p>​    Suppose $X$ and $Y$ is continuous random variables and let $Z=XY$, then the pdf of $Z$ is  </p>
<script type="math/tex; mode=display">
f_Z(z) = \int_{-\infty}^\infty \frac1{|x|}f_{X,Y}(x,\frac{z}{x})\ dx</script><p>​    If we let $Z=\frac{X}{Y}$, the pdf of $Z$ is </p>
<script type="math/tex; mode=display">
f_Z(z) = \int_{-\infty}^\infty f_{X,Y}(zy,y)|y|\ dy</script><h6 id="Theorem-3-18"><a href="#Theorem-3-18" class="headerlink" title="Theorem 3.18."></a>Theorem 3.18.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables with joint pdf  $f_X$ and supported on $\mathcal{S} \subset \mathbb{R}^n$, Define </p>
<script type="math/tex; mode=display">
Y_j = r_j(\mathbf{X})</script><p>for $j=1,\cdots,n$. Here, we assume that $r_j$ define a one-to-one differentiable transformation of $\mathcal{S}$ onto a subset $\mathcal{T}$ of $\mathbb{R}^n$. Let this inverse of this transformation be given as </p>
<script type="math/tex; mode=display">
x_i = s_i(\mathbf{y})</script><p>for $i=1,\cdots,n$. Then the joint pdf of $\mathbf{Y}$ is </p>
<script type="math/tex; mode=display">
f_{\mathbf{Y}}(\mathbf{y}) = \cases{f_{\mathbf{X}}(s)|J|\hspace{1cm}&for $\mathbf{y}\in\mathcal{T}$ \\0&otherwise}</script><p>where $J$ is the Jacobian determinant </p>
<script type="math/tex; mode=display">
J = \det\begin{bmatrix}
\frac{\partial s_1}{\partial y_1}&\cdots&\frac{\partial s_1}{\partial y_n}\\
\vdots&&\vdots\\
\frac{\partial s_n}{\partial y_1}&\cdots&\frac{\partial s_n}{\partial y_n}
\end{bmatrix}</script><p>​    Note: The condition $r_j$ is defined a one-to-one differentiable transformation is important. If the $r_j$ isn’t one-to-one, it is always wrong because we ignore some information in the transformation.</p>
<h4 id="3-11-Markov-Chains"><a href="#3-11-Markov-Chains" class="headerlink" title="3.11. Markov Chains."></a>3.11. Markov Chains.</h4><p>​    This section will not appear in my test. And the difficult part is how to construct the state instead of calculate.</p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>捏马这一章的东西实在是太多了. 打了好几天才打完…</p>
<p>首先我们找到了一个map使得sample space上的event可以被映射到$\mathbb{R}$上, 定义这个map叫做 random variable. 在定义了random variable之后我们就可以在$\mathbb{R}$上研究概率, 如此便可以定义一系列的cdf,pdf,以及joint cdf, pdf, marginal cdf, pdf. 另一方面, 类似事件独立我们可以定义随机变量的独立, 这种独立关系又可以等价到pdf与joint pdf的关系上(这是比较显然的).</p>
<p>在此之后, 对于随机变量的组合变换, 我们可以利用cdf $\Rightarrow$ 概率 $\Rightarrow$ cdf‘ $\Rightarrow$ pdf’ 推导单一随机变量的新的cdf，对于联合分布可以使用雅可比行列式来处理, 本质都是从某个support上的某个微元映射到其他support上, 这个结果虽然不好证明但是非常自然的.</p>
]]></content>
      <categories>
        <category>Probability and Statistics</category>
      </categories>
      <tags>
        <tag>Study</tag>
      </tags>
  </entry>
  <entry>
    <title>PS chapter 8</title>
    <url>/2021/12/02/PS-chapter-8/</url>
    <content><![CDATA[<script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="8-Sampling-Distributions-of-Estimators"><a href="#8-Sampling-Distributions-of-Estimators" class="headerlink" title="8. Sampling Distributions of Estimators"></a>8. Sampling Distributions of Estimators</h3><h4 id="8-1-Sampling-Distribution-of-a-Statistic"><a href="#8-1-Sampling-Distribution-of-a-Statistic" class="headerlink" title="8.1 Sampling Distribution of a Statistic."></a>8.1 Sampling Distribution of a Statistic.</h4><h6 id="Definition-8-1"><a href="#Definition-8-1" class="headerlink" title="Definition 8.1."></a>Definition 8.1.</h6><p>Suppose that the random variables <script type="math/tex">\mathbf{X}</script> form a random sample from a distribution involving an unknown parameter $\theta$. Let $T$ be a function of <script type="math/tex">\mathbf{X}</script> and possibly $\theta$. That is, $T = r(X_1,\cdots,X_n,\theta)$. The distribution of $T$ (given $\theta$) is called the <strong>sampling distribution</strong> of $T$.</p>
<p>​    We will use the notation $\mathbf{E}_{\theta}(T)$ to denote the mean of $T$ calculated from its sampling distribution</p>
<h4 id="8-2-The-Chi-Square-Distribution"><a href="#8-2-The-Chi-Square-Distribution" class="headerlink" title="8.2. The Chi-Square Distribution"></a>8.2. The Chi-Square Distribution</h4><h6 id="Definition-8-2"><a href="#Definition-8-2" class="headerlink" title="Definition 8.2."></a>Definition 8.2.</h6><p>For each positive number $m$, $\Gamma(\frac{m}{2}, \frac12)$ is called the $\chi^2$ <strong>distribution with $m$ degrees of freedom</strong>.</p>
<p>​    The pdf of $X\sim \chi^2(m)$ is </p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2^{\frac{m}2}\Gamma(\frac{m}2)} x^{\frac{m}2 - 1} e^{-\frac{x}{2}}</script><p>for $x&gt;0$ and zero elsewhere.</p>
<h6 id="Theorem-8-1-Mean-and-Variance"><a href="#Theorem-8-1-Mean-and-Variance" class="headerlink" title="Theorem 8.1 (Mean and Variance)."></a>Theorem 8.1 (Mean and Variance).</h6><p>If $X \sim \chi^2(m)$, then <script type="math/tex">\E{X} =m</script> and <script type="math/tex">\Var(X) = 2m</script>.</p>
<p>$Proof.$ <script type="math/tex">\E{X} = \frac{\alpha}{\beta}</script>, <script type="math/tex">\Var(X) = \frac{\alpha}{\beta^2}</script> </p>
<h6 id="Theorem-8-2"><a href="#Theorem-8-2" class="headerlink" title="Theorem 8.2."></a>Theorem 8.2.</h6><p>If $X_i$ are independent random variables and $X_i \sim \chi^2(m_i)$, then the sum $Y = \sum X_i$ follows the distribution $\chi^2(\sum m_i)$. </p>
<p>$Proof.$ Use mgf.</p>
<h6 id="Theorem-8-3"><a href="#Theorem-8-3" class="headerlink" title="Theorem 8.3."></a>Theorem 8.3.</h6><p>Let $X \sim N(0,1)$. Then $Y = X^2\sim \chi^2(1)$ </p>
<p>$Proof.$ Find pdf of $Y$.</p>
<h6 id="Corollary-1"><a href="#Corollary-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>If $X_1,\cdots,X_m$ are i.i.d. with the standard normal distribution, then the sum of squares $\sum X_i^2$ has the $\chi ^2$ distribution with $m$ degrees of freedom.</p>
<h6 id="Theorem-8-4"><a href="#Theorem-8-4" class="headerlink" title="Theorem 8.4."></a>Theorem 8.4.</h6><p>Suppose that $X_1,\cdots,X_n$ form a random sample from $N(\mu,\sigma^2)$. Then the sample mean $\overline{X_n}$ and the sample variance $S_n^2 = \frac{1}{n-1}\sum(X_i-\overline{X_n})^2$ are independent random variables. Further, $\overline{X_n}$ follows $N(\mu, \sigma^2/n)$ and $\sum_{i=1}^n (X_i-\overline{X_n})^2/\sigma^2$ follows $\chi^2(n-1)$</p>
<p>$Proof.$  Firstly we have $\overline{X_n}\sim N(\mu,\sigma^2/n)$. And  the proof of the independence of $\overline{X_n}$ and $S_n^2$  is out of the scope of this course. We consider the random variable</p>
<script type="math/tex; mode=display">
V = \sum_{i=1}^n \left(\frac{X_i-\mu}{\sigma}\right)^2</script><p>From Theorem 8.2 we know that $V \sim \chi^2(n)$. And </p>
<script type="math/tex; mode=display">
\begin{align*}
V &= \sum_{i=1}^n \left(\frac{(X_i-\overline{X_n})+(\overline{X_n}-\mu)}{\sigma}\right)^2\\
&= \sum_{i=1}^n \left(\frac{X_i-\overline{X_n}}{\sigma}\right)^2 + \left(\frac{\overline{X_n}-\mu}{\sigma}\right)^2 + 2\left(\frac{X_i-\overline{X_n}}{\sigma}\right) \cdot \left(\frac{\overline{X_n}-\mu}{\sigma}\right)\\
&=  \sum_{i=1}^n \left(\frac{X_i-\overline{X_n}}{\sigma}\right)^2 + \left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2\\
&= \frac{(n-1)S_n^2}{\sigma^2}+\left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2
\end{align*}</script><p>Consider $V\sim \chi^2(n)$ and $\left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2 \sim \chi^2(1)$ . In addition to $S_n^2$ and $\overline{X_n}$ is independent. We can calculate the mgf of $\frac{(n-1)S_n^2}{\sigma^2}$ and we can prove this question.</p>
<p><strong>Remark</strong>. Indeed, the sample mean and the sample variance are independent <strong>only when</strong> the random sample is <strong>drawn from a Normal Distribution</strong>.</p>
<h4 id="8-3-The-t-Distributions"><a href="#8-3-The-t-Distributions" class="headerlink" title="8.3. The t-Distributions."></a>8.3. The t-Distributions.</h4><p>When out data are a sample from the normal distribution with mean $\mu$ and variance $\sigma^2$, the distribution of $Z$ = $n^{1/2}(\hat{\mu}-\mu)/\sigma$ is the standard normal distribution, where $\hat{\mu}$ is the sample mean. If $\sigma^2$ is unknown, we can replace $\sigma^2$ by an estimator (sample variance) in the formula for $Z$. The resulting random variable has the $t$ distribution with $n-1$ degrees of freedom and is useful for making inferences about $\mu$ alone even when both $\mu$ and $\sigma^2$ are unknown.</p>
<h6 id="Definition-8-3-t-Distributions"><a href="#Definition-8-3-t-Distributions" class="headerlink" title="Definition 8.3 (t Distributions)"></a>Definition 8.3 (t Distributions)</h6><p>Consider two independent random variables $Y$ and $Z$, such that $Y \sim \chi^2(m)$ and $Z\sim N(0,1)$. Suppose that $X$ is defined as </p>
<script type="math/tex; mode=display">
X =  \frac{Z}{(Y/m)^{1/2}}</script><p>Then the distribution of $X$ is called the <strong>t distribution with $m$ degree of freedom</strong>.</p>
<h6 id="Theorem-8-5"><a href="#Theorem-8-5" class="headerlink" title="Theorem 8.5."></a>Theorem 8.5.</h6><p>The pdf of the distribution with $m$ degrees of freedom is </p>
<script type="math/tex; mode=display">
f(x) = \frac{\Gamma(\frac{m+1}2)}{(m\pi)^{1/2}\Gamma(\frac{m}2)}(1+\frac{x^2}{m})^{-(m+1)/2}</script><p>for $-\infty &lt; x &lt; \infty$</p>
<p>​    When $m\leq 1$, the mean value of t distribution doesn’t exist. When $m&gt;1$, by the symmetry the mean value is always 0.</p>
<p>​    In general, if $X$ follows the t distribution with $m$  degrees of freedom, then the k-th absolute value moments <script type="math/tex">\E{|X|^k}<\infty</script> are finite for $k&lt;m$ and it is unbounded for $k\geq m$. Therefore, if m is an integer, the first $m-1$ moments of $X$ exists. Thus, the moment generating function of $X$ doesn’t exist.</p>
<h6 id="Theorem-8-6"><a href="#Theorem-8-6" class="headerlink" title="Theorem 8.6."></a>Theorem 8.6.</h6><p>Suppose that $X_1,\cdots,X_n$ form a random sample from the normal distribution with mean $\mu$ and $\sigma^2$, Let $\overline{X_n}$ denote the sample mean, and define </p>
<script type="math/tex; mode=display">
S_n = \left(\frac{\sum_{i=1}^n (X_i-\overline{X_n})}{n-1}\right)^{1/2}</script><p>Then $n^{1/2}(\overline{X_n}-\mu)/S_n$ has the t distribution with $n-1$ degrees of freedom.</p>
<p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\sqrt{n}(\overline{X_n}-\mu)/\sigma \sim N(0,1)\hspace{1cm}and\hspace{1cm}S_n^2/\sigma^2\sim\chi^2(n-1)</script><p>Follows the definition of the t distribution.</p>
<h3 id="8-4-The-F-Distributions"><a href="#8-4-The-F-Distributions" class="headerlink" title="8.4. The F-Distributions."></a>8.4. The F-Distributions.</h3><h6 id="Definition-8-4"><a href="#Definition-8-4" class="headerlink" title="Definition 8.4."></a>Definition 8.4.</h6><p>Let $Y$ and $W$ be independent random variables that have $\chi^2$ distribution with degrees of freedom $m$ and $n$ respectively. Define a new random variable </p>
<script type="math/tex; mode=display">
X = \frac{Y/m}{W/n}</script><p>Then the distribution of $X$ is called the F <strong>distribution with $m$ and $n$ degrees of freedom</strong>.</p>
<h6 id="Theorem-8-7"><a href="#Theorem-8-7" class="headerlink" title="Theorem 8.7."></a>Theorem 8.7.</h6><p>Let $X$ have the $F$ distribution with $m$ and $n$ degrees of freedom. Then its pdf </p>
<script type="math/tex; mode=display">
f_X(x) = \frac{\Gamma(\frac12(m+n))m^{m/2}n^{n/2}}{\Gamma(\frac12 m)\Gamma(\frac12n)} \cdot \frac{x^{(m/2)-1}}{(mx+n)^{(m+n)/2}}</script><p>for $x&gt;0$ and zero elsewhere.</p>
<h6 id="Theorem-8-8"><a href="#Theorem-8-8" class="headerlink" title="Theorem 8.8."></a>Theorem 8.8.</h6><p>If $X$ has the $F$ distribution with $m$ and $n$ degrees of freedom, then its reciprocal $1/X$ has the $F$ distribution with $n$ and $m$ degrees of freedom. If $Y$ has the $t$ distribution with $n$ degrees of freedom, then $Y^2$ has the $F$ distribution with 1 and $n$ degrees of freedom.</p>
]]></content>
      <categories>
        <category>Probability and Statistics</category>
      </categories>
      <tags>
        <tag>Study</tag>
      </tags>
  </entry>
</search>
