<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  
  

  <title>Yewandou's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yewandou's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    

  <a href="https://github.com/Yewandou7" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/10/%E4%BE%9D%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/10/%E4%BE%9D%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97/" class="post-title-link" itemprop="url">依概率分布的一些判定</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-10 16:39:40 / 修改时间：18:21:14" itemprop="dateCreated datePublished" datetime="2021-12-10T16:39:40+08:00">2021-12-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-and-Statistics/" itemprop="url" rel="index"><span itemprop="name">Probability and Statistics</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pc}{\stackrel{p}{\longrightarrow}}
\newcommand{\dc}{\stackrel{d}{\longrightarrow}}</script><script type="math/tex; mode=display">
</script><p>123</p>
<script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pc}{\stackrel{p}{\longrightarrow}}
\newcommand{\dc}{\stackrel{d}{\longrightarrow}}</script><p>最近做作业里用到了依概率分布的四则运算, 不过并没有找到证明, 自己查了查搞了搞算是会用了. 我们要考虑两个随机变量列$X_n$和$Y_n$, 设其收敛到$X$和$Y$, 即</p>
<script type="math/tex; mode=display">
\begin{align*}
X_n &\pc X\\
Y_n &\pc y
\end{align*}</script><h6 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h6><p>若<script type="math/tex">X_n \pc X</script>, <script type="math/tex">Y_n \pc Y</script>, 则<script type="math/tex">X_n+Y_n \pc X+Y</script></p>
<p>$Proof.$考虑绝对值不等式, 有</p>
<script type="math/tex; mode=display">
|(X_n+Y_n) - (X+Y)| \le |X_n-X|+|Y_n-Y|</script><p>考虑概率函数$P$是单调的, 对于任意给定的$\varepsilon&gt;0$, 都有</p>
<script type="math/tex; mode=display">
\begin{align*}
\P{|(X_n+Y_n)-(X+Y)| \geq \varepsilon} &\le \P{|X_n-X|+|Y_n-Y|\ge \varepsilon}\\
&\le \P{|X_n-X|\ge \frac{\varepsilon}{2}} + \P{|Y_n-Y|\ge \frac{\varepsilon}{2}}
\end{align*}</script><p>由于后两项是趋于0的, 所以不等式左侧是趋于0的, 所以说<script type="math/tex">X_n+Y_n \pc X+Y</script>, 证毕.</p>
<h6 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h6><p>若<script type="math/tex">X_n \pc X</script>, <script type="math/tex">Y_n \pc Y</script>, 则<script type="math/tex">X_n-Y_n \pc X-Y</script>.</p>
<p>$Proof.$ 同理定理1, 略.</p>
<h6 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h6><p>若<script type="math/tex">X_n \pc X</script>, 且函数$f$连续, 则<script type="math/tex">f(X_n) \pc f(X)</script></p>
<p>$Proof$. 对$\forall \delta&gt;0$, $\exists T&gt;0$, 使得当$|X|&gt;\frac{T}2$时, 有<script type="math/tex">\P{|X|>\frac{T}2} < \frac\delta6</script>. 又对$\frac{T}{2}$, $\exists N_1$, 当$n&gt;N_1$时, 有<script type="math/tex">\P{|X_n-X|>\frac{T}{2}} < \frac{\delta}6</script>, 则当$n&gt;N_1$时有</p>
<script type="math/tex; mode=display">
\P{|X_n| \geq T}\le \P{|X_n-X|>\frac{T}2} + \P{|X|>\frac{T}2}< \frac{\delta}{3}</script><p>考虑到$f$在$[-T,T]$上一致连续, 则对$\varepsilon&gt;0$, $\exists \varepsilon_1&gt;0$, 使当$x_1,x_2\in[-T,T]$且$|x_1-x_2|&lt;\varepsilon_1$时, 有$|f(x_1)-f(x_2)|&lt;\varepsilon$. 则当$|f(x_2)-f(x_1)|\geq \varepsilon$时, $|x_2-x_1|\geq \varepsilon_1$. 则</p>
<script type="math/tex; mode=display">
\begin{align*}
\P{|f(X_n)-f(X)|\geq \varepsilon} \le& \P{|f(X_n)-f(X)|\ge \varepsilon\big||X|, |X_n|<T} \\
&+\P{|X|>T} + \P{|X_n|>T}
\end{align*}</script><p>接下来考虑每一项, <script type="math/tex">\P{|X|>T}</script>和<script type="math/tex">\P{|X_n|>T}</script>都通过上面的方式控制在了$\delta$内, 我们只用考虑第一项, 我们有</p>
<script type="math/tex; mode=display">
\begin{align*}
\P{|f(X_n)-f(X)|\ge \varepsilon\big||X|, |X_n|<T} =\P{|X-X_n|\geq \varepsilon_1}
\end{align*}</script><p>右侧可以通过控制$n$使得这一项也在$\delta$内, 我们就证明了</p>
<script type="math/tex; mode=display">
\lim\limits_{n\to\infty} \P{|f(X_n)-f(X)|\ge \varepsilon} =0</script><p>即<script type="math/tex">f(X_n) \pc f(X)</script>, 证毕.</p>
<h6 id="定理4"><a href="#定理4" class="headerlink" title="定理4"></a>定理4</h6><p>若<script type="math/tex">X_n\pc X</script>, <script type="math/tex">Y_n \pc Y</script>, 则<script type="math/tex">X_nY_n\pc XY</script>.</p>
<p>$Proof.$ 考虑</p>
<script type="math/tex; mode=display">
X_nY_n = \frac12X_n^2+\frac12Y_n^2-\frac12(X_n-Y_n)^2</script><p>应用定理3考虑$f(\cdot) =\cdot^2$, 证毕.</p>
<h6 id="定理5"><a href="#定理5" class="headerlink" title="定理5"></a>定理5</h6><p>若<script type="math/tex">X_n\pc X</script>, <script type="math/tex">Y_n \pc Y</script>, 则<script type="math/tex">X_n/Y_n \pc X/Y</script>. <script type="math/tex">(Y, Y_n\neq 0)</script></p>
<p>$Proof.$ 我们可以先证明<script type="math/tex">1/Y_n \pc 1/Y</script>, 应用定理3考虑$f(\cdot)=\frac1\cdot$, 引理得证. 再运用定理4, 则原命题得证.</p>
<hr>
<p>至此, 我们便完成了对依概率分布四则运算的证明, 有亿点麻烦, 但结论才是重要的.</p>
<hr>
<p>我们的问题实际上是判别一个estimator是否是consistent的, 除去定义和四则运算之外, 我们还有一个很有用的定理是</p>
<h6 id="定理6"><a href="#定理6" class="headerlink" title="定理6"></a>定理6</h6><p>对于给定的$X_n$, $X$ 若满足</p>
<script type="math/tex; mode=display">
\lim\limits_{n\to\infty} \E{X_n} = X\hspace{1cm}and\hspace{1cm}\lim\limits_{n\to\infty}\Var(X_n)=0</script><p>则有<script type="math/tex">X_n \pc X</script></p>
<p>$Proof.$ 我们应用切比雪夫不等式, 有</p>
<script type="math/tex; mode=display">
\P{|X_n-\E{X_n}| \geq \frac\varepsilon{2}} \leq \frac{4\Var(X_n)}{\varepsilon^2}</script><p>我们考虑</p>
<script type="math/tex; mode=display">
|X_n-X|\le |X_n-\E{X_n}| + |\E{X_n}-X|</script><p>当<script type="math/tex">|X_n-\E{X_n}|<\frac{\varepsilon}2</script>时, 我们可以控制$n$使得<script type="math/tex">|\E{X_n}-X|<\frac\varepsilon2</script>, 那么相当于是对于任意给定的$\varepsilon$, 对于充分大的$n$而言, 只要<script type="math/tex">|X_n-\E{X_n}|<\frac{\varepsilon}2</script>, 一定有$|X_n-X|&lt;\varepsilon$, 那么就说明了对充分大的$n$</p>
<script type="math/tex; mode=display">
\{|X_n-X|\ge \varepsilon\} \subset\{|X_n-\E{X_n}|\ge\varepsilon\}</script><p>那么</p>
<script type="math/tex; mode=display">
\P{|X_n-X|\geq\varepsilon}\le\P{|X_n-\E{X_n}|\ge\frac\varepsilon2} \le \frac{4\Var(X_n)}{\varepsilon^2}</script><p>即证明了</p>
<script type="math/tex; mode=display">
\lim_{n\to\infty} \P{|X_n-X|\ge\varepsilon} =0</script><p>即</p>
<script type="math/tex; mode=display">
X_n\pc X</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/02/PS-chapter-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/02/PS-chapter-8/" class="post-title-link" itemprop="url">PS chapter 8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-02 10:19:19" itemprop="dateCreated datePublished" datetime="2021-12-02T10:19:19+08:00">2021-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-10 16:44:26" itemprop="dateModified" datetime="2021-12-10T16:44:26+08:00">2021-12-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-and-Statistics/" itemprop="url" rel="index"><span itemprop="name">Probability and Statistics</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="8-Sampling-Distributions-of-Estimators"><a href="#8-Sampling-Distributions-of-Estimators" class="headerlink" title="8. Sampling Distributions of Estimators"></a>8. Sampling Distributions of Estimators</h3><h4 id="8-1-Sampling-Distribution-of-a-Statistic"><a href="#8-1-Sampling-Distribution-of-a-Statistic" class="headerlink" title="8.1 Sampling Distribution of a Statistic."></a>8.1 Sampling Distribution of a Statistic.</h4><h6 id="Definition-8-1"><a href="#Definition-8-1" class="headerlink" title="Definition 8.1."></a>Definition 8.1.</h6><p>Suppose that the random variables <script type="math/tex">\mathbf{X}</script> form a random sample from a distribution involving an unknown parameter $\theta$. Let $T$ be a function of <script type="math/tex">\mathbf{X}</script> and possibly $\theta$. That is, $T = r(X_1,\cdots,X_n,\theta)$. The distribution of $T$ (given $\theta$) is called the <strong>sampling distribution</strong> of $T$.</p>
<p>​    We will use the notation $\mathbf{E}_{\theta}(T)$ to denote the mean of $T$ calculated from its sampling distribution</p>
<hr>
<h4 id="8-2-The-Chi-Square-Distribution"><a href="#8-2-The-Chi-Square-Distribution" class="headerlink" title="8.2. The Chi-Square Distribution"></a>8.2. The Chi-Square Distribution</h4><h6 id="Definition-8-2"><a href="#Definition-8-2" class="headerlink" title="Definition 8.2."></a>Definition 8.2.</h6><p>For each positive number $m$, $\Gamma(\frac{m}{2}, \frac12)$ is called the $\chi^2$ <strong>distribution with $m$ degrees of freedom</strong>.</p>
<p>​    The pdf of $X\sim \chi^2(m)$ is </p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2^{\frac{m}2}\Gamma(\frac{m}2)} x^{\frac{m}2 - 1} e^{-\frac{x}{2}}</script><p>for $x&gt;0$ and zero elsewhere.</p>
<h6 id="Theorem-8-1-Mean-and-Variance"><a href="#Theorem-8-1-Mean-and-Variance" class="headerlink" title="Theorem 8.1 (Mean and Variance)."></a>Theorem 8.1 (Mean and Variance).</h6><p>If $X \sim \chi^2(m)$, then <script type="math/tex">\E{X} =m</script> and <script type="math/tex">\Var(X) = 2m</script>.</p>
<p>$Proof.$ <script type="math/tex">\E{X} = \frac{\alpha}{\beta}</script>, <script type="math/tex">\Var(X) = \frac{\alpha}{\beta^2}</script> </p>
<h6 id="Theorem-8-2"><a href="#Theorem-8-2" class="headerlink" title="Theorem 8.2."></a>Theorem 8.2.</h6><p>If $X_i$ are independent random variables and $X_i \sim \chi^2(m_i)$, then the sum $Y = \sum X_i$ follows the distribution $\chi^2(\sum m_i)$. </p>
<p>$Proof.$ Use mgf.</p>
<h6 id="Theorem-8-3"><a href="#Theorem-8-3" class="headerlink" title="Theorem 8.3."></a>Theorem 8.3.</h6><p>Let $X \sim N(0,1)$. Then $Y = X^2\sim \chi^2(1)$ </p>
<p>$Proof.$ Find pdf of $Y$.</p>
<h6 id="Corollary-1"><a href="#Corollary-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>If $X_1,\cdots,X_m$ are i.i.d. with the standard normal distribution, then the sum of squares $\sum X_i^2$ has the $\chi ^2$ distribution with $m$ degrees of freedom.</p>
<h6 id="Theorem-8-4"><a href="#Theorem-8-4" class="headerlink" title="Theorem 8.4."></a>Theorem 8.4.</h6><p>Suppose that $X_1,\cdots,X_n$ form a random sample from $N(\mu,\sigma^2)$. Then the sample mean $\overline{X_n}$ and the sample variance $S_n^2 = \frac{1}{n-1}\sum(X_i-\overline{X_n})^2$ are independent random variables. Further, $\overline{X_n}$ follows $N(\mu, \sigma^2/n)$ and $\sum_{i=1}^n (X_i-\overline{X_n})^2/\sigma^2$ follows $\chi^2(n-1)$</p>
<p>$Proof.$  Firstly we have $\overline{X_n}\sim N(\mu,\sigma^2/n)$. And  the proof of the independence of $\overline{X_n}$ and $S_n^2$  is out of the scope of this course. We consider the random variable</p>
<script type="math/tex; mode=display">
V = \sum_{i=1}^n \left(\frac{X_i-\mu}{\sigma}\right)^2</script><p>From Theorem 8.2 we know that $V \sim \chi^2(n)$. And </p>
<script type="math/tex; mode=display">
\begin{align*}
V &= \sum_{i=1}^n \left(\frac{(X_i-\overline{X_n})+(\overline{X_n}-\mu)}{\sigma}\right)^2\\
&= \sum_{i=1}^n \left(\frac{X_i-\overline{X_n}}{\sigma}\right)^2 + \left(\frac{\overline{X_n}-\mu}{\sigma}\right)^2 + 2\left(\frac{X_i-\overline{X_n}}{\sigma}\right) \cdot \left(\frac{\overline{X_n}-\mu}{\sigma}\right)\\
&=  \sum_{i=1}^n \left(\frac{X_i-\overline{X_n}}{\sigma}\right)^2 + \left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2\\
&= \frac{(n-1)S_n^2}{\sigma^2}+\left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2
\end{align*}</script><p>Consider $V\sim \chi^2(n)$ and $\left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}\right)^2 \sim \chi^2(1)$ . In addition to $S_n^2$ and $\overline{X_n}$ is independent. We can calculate the mgf of $\frac{(n-1)S_n^2}{\sigma^2}$ and we can prove this question.</p>
<p><strong>Remark</strong>. Indeed, the sample mean and the sample variance are independent <strong>only when</strong> the random sample is <strong>drawn from a Normal Distribution</strong>.</p>
<hr>
<h4 id="8-3-The-t-Distributions"><a href="#8-3-The-t-Distributions" class="headerlink" title="8.3. The t-Distributions."></a>8.3. The t-Distributions.</h4><p>When out data are a sample from the normal distribution with mean $\mu$ and variance $\sigma^2$, the distribution of $Z$ = $n^{1/2}(\hat{\mu}-\mu)/\sigma$ is the standard normal distribution, where $\hat{\mu}$ is the sample mean. If $\sigma^2$ is unknown, we can replace $\sigma^2$ by an estimator (sample variance) in the formula for $Z$. The resulting random variable has the $t$ distribution with $n-1$ degrees of freedom and is useful for making inferences about $\mu$ alone even when both $\mu$ and $\sigma^2$ are unknown.</p>
<h6 id="Definition-8-3-t-Distributions"><a href="#Definition-8-3-t-Distributions" class="headerlink" title="Definition 8.3 (t Distributions)"></a>Definition 8.3 (t Distributions)</h6><p>Consider two independent random variables $Y$ and $Z$, such that $Y \sim \chi^2(m)$ and $Z\sim N(0,1)$. Suppose that $X$ is defined as </p>
<script type="math/tex; mode=display">
X =  \frac{Z}{(Y/m)^{1/2}}</script><p>Then the distribution of $X$ is called the <strong>t distribution with $m$ degree of freedom</strong>.</p>
<h6 id="Theorem-8-5"><a href="#Theorem-8-5" class="headerlink" title="Theorem 8.5."></a>Theorem 8.5.</h6><p>The pdf of the t distribution with $m$ degrees of freedom is </p>
<script type="math/tex; mode=display">
f(x) = \frac{\Gamma(\frac{m+1}2)}{(m\pi)^{1/2}\Gamma(\frac{m}2)}(1+\frac{x^2}{m})^{-(m+1)/2}</script><p>for $-\infty &lt; x &lt; \infty$</p>
<p>​    When $m\leq 1$, the mean value of t distribution doesn’t exist. When $m&gt;1$, by the symmetry the mean value is always 0.</p>
<p>​    In general, if $X$ follows the t distribution with $m$  degrees of freedom, then the k-th absolute value moments <script type="math/tex">\E{|X|^k}<\infty</script> are finite for $k&lt;m$ and it is unbounded for $k\geq m$. Therefore, if m is an integer, the first $m-1$ moments of $X$ exists. Thus, the moment generating function of $X$ doesn’t exist.</p>
<h6 id="Theorem-8-6"><a href="#Theorem-8-6" class="headerlink" title="Theorem 8.6."></a>Theorem 8.6.</h6><p>Suppose that $X_1,\cdots,X_n$ form a random sample from the normal distribution with mean $\mu$ and $\sigma^2$, Let $\overline{X_n}$ denote the sample mean, and define </p>
<script type="math/tex; mode=display">
S_n = \left(\frac{\sum_{i=1}^n (X_i-\overline{X_n})^2}{n-1}\right)^{1/2}</script><p>Then $n^{1/2}(\overline{X_n}-\mu)/S_n$ has the t distribution with $n-1$ degrees of freedom.</p>
<p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\sqrt{n}(\overline{X_n}-\mu)/\sigma \sim N(0,1)\hspace{1cm}and\hspace{1cm}(n-1)S_n^2/\sigma^2\sim\chi^2(n-1)</script><p>Then </p>
<script type="math/tex; mode=display">
\frac{\sqrt{n}(\overline{X_n}-\mu)}{S_n} = \frac{\sqrt{n}(X_n-\mu)/\sigma}{\sqrt{\frac{(n-1)S_n^2}{\sigma^2}/(n-1)}}\sim \frac{N(0,1)}{\chi^2(n-1)}</script><p>Follows the definition of the t distribution.</p>
<hr>
<h4 id="8-4-The-F-Distributions"><a href="#8-4-The-F-Distributions" class="headerlink" title="8.4. The F-Distributions."></a>8.4. The F-Distributions.</h4><h6 id="Definition-8-4"><a href="#Definition-8-4" class="headerlink" title="Definition 8.4."></a>Definition 8.4.</h6><p>Let $Y$ and $W$ be independent random variables that have $\chi^2$ distribution with degrees of freedom $m$ and $n$ respectively. Define a new random variable </p>
<script type="math/tex; mode=display">
X = \frac{Y/m}{W/n}</script><p>Then the distribution of $X$ is called the F <strong>distribution with $m$ and $n$ degrees of freedom</strong>.</p>
<h6 id="Theorem-8-7"><a href="#Theorem-8-7" class="headerlink" title="Theorem 8.7."></a>Theorem 8.7.</h6><p>Let $X$ have the $F$ distribution with $m$ and $n$ degrees of freedom. Then its pdf </p>
<script type="math/tex; mode=display">
f_X(x) = \frac{\Gamma(\frac12(m+n))m^{m/2}n^{n/2}}{\Gamma(\frac12 m)\Gamma(\frac12n)} \cdot \frac{x^{(m/2)-1}}{(mx+n)^{(m+n)/2}}</script><p>for $x&gt;0$ and zero elsewhere.</p>
<h6 id="Theorem-8-8"><a href="#Theorem-8-8" class="headerlink" title="Theorem 8.8."></a>Theorem 8.8.</h6><p>If $X$ has the $F$ distribution with $m$ and $n$ degrees of freedom, then its reciprocal $1/X$ has the $F$ distribution with $n$ and $m$ degrees of freedom. If $Y$ has the $t$ distribution with $n$ degrees of freedom, then $Y^2$ has the $F$ distribution with 1 and $n$ degrees of freedom.</p>
<p>$Proof.$ The first part is </p>
<script type="math/tex; mode=display">
X=\frac{Y/m}{Z/n}\Rightarrow 1/X = \frac{Z/n}{Y/m}</script><p>which means $1/X $ is a $F$ distribution with $n$ and $m$ distribution.</p>
<p>The second part is </p>
<script type="math/tex; mode=display">
Y = \frac{X}{(Z/n)^{1/2}} \Rightarrow Y^2 = \frac{X^2}{Z/n}</script><p>where $X\sim N(0,1)$ thus $X^2 \sim \chi^2(1)$, and $Y^2$ is a $F$ distribution with $n$ and $m$ distribution.</p>
<hr>
<h4 id="8-5-Confidence-Intervals"><a href="#8-5-Confidence-Intervals" class="headerlink" title="8.5. Confidence Intervals."></a>8.5. Confidence Intervals.</h4><h6 id="Definition-8-5-Confidence-Interval"><a href="#Definition-8-5-Confidence-Interval" class="headerlink" title="Definition 8.5 (Confidence Interval)."></a>Definition 8.5 (Confidence Interval).</h6><p>Let $X_1,X_2,\cdots,X_n$ be a sample on a random variable $X$, where $X$ has the pdf $f(x;\theta)$, $\theta \in \Omega$. Let $\alpha \in (0,1)$ be specified. Let $L=L(X_1,\cdots,X_n)$ and $U=U(X_1,\cdots,X_n)$ be two statistics and let $g(\cdot)$ be a real function. We say that the interval $(L,U)$ is a $(1-\alpha)\times100\%$ <strong>confidence interval</strong> for $g(\theta)$ if </p>
<script type="math/tex; mode=display">
P_{\theta}(L<g(\theta)\le U) \ge 1-\alpha</script><p>Which means the probability that the interval $(L,U)$ contains $g(\theta)$ is $1-\alpha$</p>
<h6 id="Theorem-8-9-For-a-Normal-Distribution-with-known-sigma-2"><a href="#Theorem-8-9-For-a-Normal-Distribution-with-known-sigma-2" class="headerlink" title="Theorem 8.9 (For a Normal Distribution with known  $\sigma^2$)"></a>Theorem 8.9 (For a Normal Distribution with known  $\sigma^2$)</h6><p>Let $(X_i)_{i=1}^n$ be a random sample from $N(\mu, \sigma^2)$. For each $0&lt;\alpha&lt;1$, the interval $(L,U)$ with the following endpoints is an exact coefficient $1-\alpha$ confidence interval for $\mu$: </p>
<script type="math/tex; mode=display">
\begin{align*}
L=\overline{X_n}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\\
R=\overline{X_n}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}
\end{align*}</script><p>where $z_{\alpha/2}$ is the value for <script type="math/tex">\P{X>z_{\alpha/2}}=\frac{\alpha}{2}</script></p>
<p>$Proof.$ Consider $\overline{X_n} \sim N(\mu,\frac{\sigma^2}{n})$, thus </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{-z_{\alpha/2} < \frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}}<z_{\alpha/2}} = 1-\alpha\\
\P{\overline{X_n}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}} <\mu < \overline{X_n}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}} = 1-\alpha
\end{align*}</script><h6 id="Theorem-8-10-For-a-Normal-Distribution-with-unknown-sigma-2"><a href="#Theorem-8-10-For-a-Normal-Distribution-with-unknown-sigma-2" class="headerlink" title="Theorem 8.10 (For a Normal Distribution with unknown $\sigma^2$)"></a>Theorem 8.10 (For a Normal Distribution with unknown $\sigma^2$)</h6><p>Let $(X_i)_{i=1}^n$ be a random sample from $N(\mu, \sigma^2)$. For each $0&lt;\alpha&lt;1$, the interval $(L,U)$ with the following endpoints is an exact coefficient $1-\alpha$ confidence interval for $\mu$: </p>
<script type="math/tex; mode=display">
\begin{align*}
L=\overline{X_n} -t_{\alpha/2,n-1}\frac{S_n}{\sqrt{n}}\\
R=\overline{X_n} +t_{\alpha/2,n-1}\frac{S_n}{\sqrt{n}}
\end{align*}</script><p>$Proof.$ From Theorem 8.6 we know </p>
<script type="math/tex; mode=display">
\frac{\sqrt{n}(\overline{X_n}-\mu)}{S_n}</script><p>follows the t distribution with degree of freedom $n-1$, thus </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{-t_{\alpha/2,n-1}<\frac{\overline{X_n}-\mu}{S_n/\sqrt{n}}<t_{\alpha/2,n-1}} = 1-\alpha\\
\P{\overline{X_n}-t_{\alpha/2,n-1}<\mu<\overline{X_n}+t_{\alpha/2,n-1}} = 1-\alpha
\end{align*}</script><h6 id="Definition-8-6-One-Sided-Confidence-Intervals"><a href="#Definition-8-6-One-Sided-Confidence-Intervals" class="headerlink" title="Definition 8.6 (One-Sided Confidence Intervals)."></a>Definition 8.6 (One-Sided Confidence Intervals).</h6><p>Let $\mathbf{X}$ be a random sample from a distribution that depends on a parameter (or parameter vector) $\theta$, Let $g(\theta)$ be a real-valued function. Let $L$ be a statistic such that for all $\theta$, </p>
<script type="math/tex; mode=display">
\P{L<g(\theta)}\ge 1-\alpha</script><p>Then the random interval $(L,\infty)$ is called <strong>one-sided coefficient</strong> $1-\alpha$ confidence interval for $g(\theta)$. Also, $L$ is called a <strong>coefficient $1-\alpha$ lower confidence limit</strong> for $g(\theta)$. Similar we can define $U$ be a <strong>coefficient $1-\alpha$ upper confidence limit</strong> for $g(\theta)$ if </p>
<script type="math/tex; mode=display">
\P{g(\theta)<U} \ge 1-\alpha</script><h6 id="Theorem-8-11-Confidence-Intervals-for-the-Variance-of-a-Normal-Distribution"><a href="#Theorem-8-11-Confidence-Intervals-for-the-Variance-of-a-Normal-Distribution" class="headerlink" title="Theorem 8.11 (Confidence Intervals for the Variance of a Normal Distribution)"></a>Theorem 8.11 (Confidence Intervals for the Variance of a Normal Distribution)</h6><p>Let $(X_i)_{i=1}^n$ be a random sample from $N(\mu,\sigma^2)$. For each $\alpha\in(0,1)$, the following statistic are lower and upper coefficient $1-\alpha$ confidence limits for $\sigma^2$ </p>
<script type="math/tex; mode=display">
\begin{align*}
L=\frac{(n-1)S_n^2}{\chi^2_{\alpha/2,n-1}}\\
R=\frac{(n-1)S_n^2}{\chi^2_{1-\alpha/2,n-1}}
\end{align*}</script><p>$Proof.$ We know that $\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)$, thus </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{\chi^2_{1-\alpha/2,n-1}< \frac{(n-1)S_n^2}{\sigma^2}<\chi^2_{\alpha/2,n-1}} = 1-\alpha\\
\P{\frac{(n-1)S_n^2}{\chi^2_{\alpha/2,n-1}}<\sigma^2<\frac{(n-1)S_n^2}{\chi^2_{1-\alpha/2,n-1}}} = 1-\alpha
\end{align*}</script><h6 id="Example-8-4-Estimating-the-Difference-in-Means-of-Two-Normal-Distribution-with-known-sigma-1-2-and-sigma-2-2"><a href="#Example-8-4-Estimating-the-Difference-in-Means-of-Two-Normal-Distribution-with-known-sigma-1-2-and-sigma-2-2" class="headerlink" title="Example 8.4 (Estimating the Difference in Means of Two Normal Distribution with known $\sigma_1^2$ and $\sigma_2^2$)"></a>Example 8.4 (Estimating the Difference in Means of Two Normal Distribution with known $\sigma_1^2$ and $\sigma_2^2$)</h6><p>Let $(X_i)_{i=1}^n$ be a random sample of size $n$ from $N(\mu_1,\sigma_1^2)$ and $(Y_i)_{i=1}^m$ be a random sample of size $m$ from $N(\mu_2,\sigma_2^2)$. We want to find the confidence interval for $\mu_1-\mu_2$.</p>
<p>Firstly we have </p>
<script type="math/tex; mode=display">
\overline{X_n}-\overline{Y_m} \sim N(\mu_1-\mu_2, \frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m})</script><p> Thus we define </p>
<script type="math/tex; mode=display">
Z = \frac{\overline{X_n}-\overline{Y_m} - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}} \sim N(0,1)</script><p>Thus </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{-z_{\alpha/2}<Z<z_{\alpha/2}} = 1-\alpha\\
L = \overline{X_n}-\overline{Y_m} - z_{\alpha/2}\sqrt{\frac{\sigma_1^2}n+\frac{\sigma_2^2}{m}}\\
U = \overline{X_n}-\overline{Y_m} + z_{\alpha/2}\sqrt{\frac{\sigma_1^2}n+\frac{\sigma_2^2}{m}}
\end{align*}</script><h6 id="Example-8-5-Estimating-the-mean-of-the-exponential-distribution"><a href="#Example-8-5-Estimating-the-mean-of-the-exponential-distribution" class="headerlink" title="Example 8.5 (Estimating the mean of the exponential distribution)"></a>Example 8.5 (Estimating the mean of the exponential distribution)</h6><p>Let $(X_i)_{i=1}^n$ be a random sample from an exponential distribution with mean $\theta$, we will try to find the two-sided coefficient $1-\alpha$ confidence interval for $\theta$. </p>
<p>$Solution.$ Firstly we can consider that $X_i \sim \exp(\theta)$, which means </p>
<script type="math/tex; mode=display">
\sum_{i=1}^n X_i \sim \Gamma(n, \frac1\theta)</script><p>Next we will prove $\frac1\theta \sum_{i=1}^n X_i \sim \Gamma(n,1)$. </p>
<script type="math/tex; mode=display">
\begin{align*}
f_X(X) &= \frac{\frac{1}{\theta}^n}{\Gamma(n)} x^{n-1}e^{-\frac{1}{\theta}x}\\
f_Y(y) &= \frac{1}{\Gamma(n)} y^{n-1} e^{-y}\hspace{1cm}\text{Let $y=\frac{x}\theta$}
\end{align*}</script><p>which means $\frac{1}{\theta}\sum_{i=1}^n X_i \sim \Gamma(n,1)\sim\chi^2(2n)$. where only $\theta$ is unknown. We can find the table of chi-square distribution to solve the question.</p>
<h6 id="Example-8-6"><a href="#Example-8-6" class="headerlink" title="Example 8.6."></a>Example 8.6.</h6><p>Let $(X_i)_{i=1}^n $ and $(Y_i)_{i=1}^m $ be two independent random samples from $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2)$ where $4$ parameters are unknown. We will try to find the confidence interval for $\sigma_1^2/\sigma_2^2$</p>
<p>$Solution.$ Firstly we have </p>
<script type="math/tex; mode=display">
\frac{(n-1)S_1^2}{\sigma_1^2} \sim \chi^2(n-1)\hspace{1cm}and\hspace{1cm}\frac{(m-1)S^2_{2}}{\sigma_2^2}\sim\chi^2(m-1)</script><p>Thus </p>
<script type="math/tex; mode=display">
\frac{\sigma_1^2}{\sigma_2^2} \cdot \frac{S_2^2}{S_1^2} \sim F(m-1,n-1)</script><p>We can solve this problem by finding the table of the $F$ distribution.</p>
<hr>
<h4 id="8-6-Order-Statistics-and-Confidence-Interval"><a href="#8-6-Order-Statistics-and-Confidence-Interval" class="headerlink" title="8.6 Order Statistics and Confidence Interval."></a>8.6 Order Statistics and Confidence Interval.</h4><p>​    Let $X_1,\cdots,X_n$ denote a random sample from a distribution of continuous type with pdf $f(x)$ and support $S$. Let $Y_1$ be the smallest of $X_i$, $Y_2$ be the next $X_i$ in order of magnitude, …, and $Y_n$ be the largest of $X_i$. We call $Y_i$, the ith <strong>order statistic</strong> of the random sample $X_i$. Then the joint pdf of $Y_1,\cdots,Y_n$ is given in the following theorem.</p>
<h6 id="Theorem-8-12"><a href="#Theorem-8-12" class="headerlink" title="Theorem 8.12."></a>Theorem 8.12.</h6><p>Let $Y_1&lt;Y_2&lt;\cdots&lt;Y_n$ denote the $n$ order statistics base on the random sample $X_i$ from a continuous type distributiion with pdf $f(x)$ and support $S=(a,b)$. The joint pdf of $Y_i$ is </p>
<script type="math/tex; mode=display">
g(y_1,\cdots,y_n) = \begin{cases}n!f(y_1)\cdots f(y_n)&a<y_1<\cdots<y_n<b\\
0&otherwise
\end{cases}</script><hr>
<h4 id="8-7-Confidence-Intervals-for-Parameters-of-Discrete-Distributions"><a href="#8-7-Confidence-Intervals-for-Parameters-of-Discrete-Distributions" class="headerlink" title="8.7 Confidence Intervals for Parameters of Discrete Distributions."></a>8.7 Confidence Intervals for Parameters of Discrete Distributions.</h4><p>In this section we try to find the exact confidence intervals for the parameters of discrete random variables. Let $X_1,\cdots,X_n$ be a random sample on a discreet random variable $X$ with pmf $p(x;\theta)$, $\theta\in\Omega$, where $\Omega$ is an interval of real numbers. Let $T=T(X_1,\cdots,X_n)$ be an estimator of $\theta$ with cdf $F_T(t;\theta)$. Assume $F_T(t;\theta)$ is a <strong>nonincreasing and continuous</strong> function of $\theta$ for every $t$ in the support of $T$. Let $\alpha_1&gt;0$ and $\alpha_2&gt;0$ be given such that $\alpha = \alpha_1+\alpha_2&lt;0.5$, Let $\underline{\theta}$ and $\overline{\theta}$ be the solutions of </p>
<script type="math/tex; mode=display">
\P{T\le t^*;\underline{\theta}} = 1-\alpha_2\hspace{1cm}and\hspace{1cm}\P{T\le t; \overline{\theta}}=\alpha_1</script><p>where $t^*&lt;t$ are the consecutive support values.</p>
<p>​    Under these conditions, the interval $(\underline\theta,\overline\theta)$ is a confidence interval for $\theta$ with confidence coefficient of at least $1-\alpha$</p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary."></a>Summary.</h4><p>这一章应该比想象中要简单一点, 主要是有很多概念. </p>
<p>首先介绍的就是卡方分布$\chi^2(n)$以及其对应的自由度, 事实上我不知道这个自由度和物理里的自由度有没有关联, 但现在在我看来就是一个$\chi^2(n) \sim \Gamma(\frac{n}{2},\frac12)$的关系. 接下来给出了一个卡方分布与正态分布之间的关系即$X\sim N(0,1)\Rightarrow X^2\sim\chi^2(1)$. 由于CLT的存在, 我们总可以对样本近似正态分布后去处理一些情况. 然后呢, 卡方分布实际上是对正态分布的变量取平方, 所以说卡方分布的作用是在方差上进行的. 对于样本方差$S_n^2$, 我们有非常重要的结论是$(n-1)S_n^2/\sigma^2 \sim \chi^2(n-1)$, 这个式子揭露了样本方差与真实方差的关系. </p>
<p>接下来引入了$t$分布, 定义为$\frac{X}{(Y/n)^{1/2}} \sim t(n)$. 同样也有一个非常重要的结论是$\sqrt{n}(\overline{X_n}-\mu)/S_n \sim t(n-1)$. 至于这个东西的用处稍等再说. </p>
<p>最后是引入了$F$分布, 定义为$\frac{X/m}{Y/n} \sim F(m,n)$. 这个分布我并不知道他有什么用(至少没学到)… 但在计算上来看, 他的$k$阶矩是易求的, 原因是卡方分布的$k$阶矩是很好求得的. 给出一个结论是</p>
<script type="math/tex; mode=display">
\E{Z^k} = \frac{n^k}{m^k}\E{X^k}\E{Y^{-k}} = \frac{n^k}{m^k} \frac{\Gamma(\frac{n}{2}+k)}{\Gamma(\frac{n}2)}\cdot \frac{\Gamma(\frac{m}2-k)}{\Gamma(\frac{m}2)}</script><p>在给出三大分布之后, 我们介绍了confidence interval这个概念. 即给出一个区间$(L,U)$使得以参数$\theta$为变量的实函数$g(\cdot)$，对给定的$1-\alpha$,有 </p>
<script type="math/tex; mode=display">
\P{L\le g(\theta)\le U} = 1-\alpha</script><p>其中$L, U$都是测量量. 即对某组$\mathbf{X}$来说, $L=L(\mathbf{X})$ 及 $U = U(\mathbf{X})$. 由于CLT的存在, 我们总能将任意的分布近似到正态分布上, 而对于正态分布来说, 如果我们要通过一组测量量$\mathbf{X}$ 来估计参数$\mu$和$\sigma^2$, 无非就三种情况</p>
<ol>
<li>$\mu$未知, $\sigma^2$已知.</li>
<li>$\mu$未知, $\sigma^2$未知.</li>
<li>$\mu$已知, $\sigma^2$未知.</li>
</ol>
<p>先梳理我们已知的几个通用信息, 首先有最基本的$\overline{X_n} \sim N(\mu,\frac{\sigma^2}{n})$，这个是我们所熟知的. 再加上我们这章所学的</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)\\
\frac{\sqrt{n}(\overline{X_n}-\mu)}{S_n} \sim t(n-1)
\end{align*}</script><p>总共有三个分布我们可以使用.</p>
<p>先考虑对$\mu$的估计. </p>
<p>​    对于情况1来说, 我们考虑正态分布</p>
<script type="math/tex; mode=display">
\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)</script><p>​    这个式子里只有$\mu$是未知量, 可以通过这个式子来给出一个$\mu$的置信区间的估计. 适用于知道$\overline{X_n}$但不知道$S_n$的情况.</p>
<p>​    对于情况2来说, 我们考虑$t$分布</p>
<script type="math/tex; mode=display">
\frac{\sqrt{n}(\overline{X_n}-\mu)}{S_n} \sim t(n-1)</script><p>​    这个式子里同样只有$\mu$是未知量, 可以通过t分布表来给出$\mu$的置信区间的估计. 适用于知道$\overline{X_n}$与$S_n$的情况, 是一种用样本方差代替总体方差的思想.</p>
<p>再考虑对$\sigma^2$的估计. </p>
<p>​    对于情况2来说, 我们考虑卡方分布</p>
<script type="math/tex; mode=display">
\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2(n-1)</script><p>​    这个式子里只有$\sigma^2$是未知量, 可以通过卡方分    布表来给出$\sigma^2$的置信区间的估计. 适用于只知道$S_n$的情况.</p>
<p>​    对于情况3来说, 我们还是可以通过正态分布</p>
<script type="math/tex; mode=display">
\frac{\overline{X_n}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)</script><p>​    给出一个$\sigma$置信区间的估计. 适用于知道$\overline{X_n}$的情况. </p>
<p>最后讲了一个对于离散随机变量参数的置信区间, 定义比较鬼畜, 总的来说就是对于给定的一个pmf $p(x;\theta)$, 给定一个$\alpha$及两个参数$\alpha_1+\alpha_2=\alpha&lt;0.5$, 对于某组测量量$\mathbf{X}$及对应的$\overline{x} = \overline{X_n}$, 设$\overline{x}-$是与$\overline{x}$相邻的值 有</p>
<script type="math/tex; mode=display">
\P{\overline{X_n}\le \overline{x}-;\underline\theta} = 1-\alpha_1\\
\P{\overline{X_n}\le \overline{x};\overline\theta} = \alpha_2</script><p>如此求得的$(\underline\theta,\overline{\theta})$便被称为参数的置信区间.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/PS-chapter-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/PS-chapter-3/" class="post-title-link" itemprop="url">PS chapter 3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-27 20:26:06" itemprop="dateCreated datePublished" datetime="2021-11-27T20:26:06+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-05 14:13:15" itemprop="dateModified" datetime="2021-12-05T14:13:15+08:00">2021-12-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-and-Statistics/" itemprop="url" rel="index"><span itemprop="name">Probability and Statistics</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="3-Random-Variables-and-Distributions"><a href="#3-Random-Variables-and-Distributions" class="headerlink" title="3. Random Variables and Distributions"></a>3. Random Variables and Distributions</h3><h4 id="3-1-Discrete-Random-Variables"><a href="#3-1-Discrete-Random-Variables" class="headerlink" title="3.1. Discrete Random Variables."></a>3.1. Discrete Random Variables.</h4><h6 id="Definition-3-1"><a href="#Definition-3-1" class="headerlink" title="Definition 3.1."></a>Definition 3.1.</h6><p>Consider a random experiment with a sample space $\Omega$. A function $X$, which assigns to each element $s \in \Omega$ one and only one number $X(s) = x$, is called a <strong>random variable</strong>. The <strong>space</strong> or <strong>range</strong> of $X$ is the set of real numbers $\mathcal{D} = \{x:x=X(s), s\in\Omega\}$.</p>
<p>​    In this text, $\mathcal{D}$ generally is a <strong>a countable  set</strong> or <strong>an interval</strong> of real numbers. We call random variables of the first type <strong>discrete</strong> random variables, while we call those of the second type <strong>continuous</strong> random variables.</p>
<p>​    Given a random variable $X$, its range $\mathcal{D}$ becomes the sample space of interest. Besides inducing the sample space $\mathcal{D}$, $X$ also induces a probability which we call the <strong>distribution</strong> of $X$.</p>
<h6 id="Definition-3-2"><a href="#Definition-3-2" class="headerlink" title="Definition 3.2."></a>Definition 3.2.</h6><p>Let $X$ be a discrete random variable that takes values in $\mathcal{D} = \{d_1,\cdots,d_m\}$. Define the function </p>
<script type="math/tex; mode=display">
p_X(d_i) = \P{\{s\in \Omega:X(s) = d_i\}},\ \ \text{for }i=1,\cdots,m</script><p>This is called the <strong>probability mass function (pmf)</strong> of $X$.</p>
<p>​    By the definition, for a discrete random variable to find the probability of an event $A$ from a pmf, we can simply calculate </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{x\in A} p_X(x)</script><h6 id="Theorem-3-1"><a href="#Theorem-3-1" class="headerlink" title="Theorem 3.1."></a>Theorem 3.1.</h6><p>Let $X$ be a discrete random variable with pmf $p_X(x)$. If $x$ is not one the possible values of $X$, then $p_X(x) = 0$. Also, if the sequence $d_1,d_2,\cdots$ includes all the possible values of $X$, then $\sum_{i=1}^\infty p_X(d_i) =1$.<br>$Proof.$ Use the fact that </p>
<script type="math/tex; mode=display">
\sum_{d_i} p_X(d_i) = \sum_{d_i} \P{X=d_i} = \P{\Omega} = 1</script><h4 id="3-2-Bernoulli-Binomial-and-Geometric-Distributions"><a href="#3-2-Bernoulli-Binomial-and-Geometric-Distributions" class="headerlink" title="3.2 Bernoulli, Binomial, and Geometric Distributions."></a>3.2 Bernoulli, Binomial, and Geometric Distributions.</h4><p>We will introduce these distributions in Chapter  5</p>
<h4 id="3-3-Continuous-Distributions"><a href="#3-3-Continuous-Distributions" class="headerlink" title="3.3 Continuous Distributions."></a>3.3 Continuous Distributions.</h4><h6 id="Definition-3-7-Continuous-Distribution"><a href="#Definition-3-7-Continuous-Distribution" class="headerlink" title="Definition 3.7 (Continuous Distribution)."></a>Definition 3.7 (Continuous Distribution).</h6><p>A random variable $X$ is continuous if for some function $f:\mathbb{R} \to \mathbb{R} $ and for any $a\le b\in \mathbb{R}$, </p>
<script type="math/tex; mode=display">
\P{a\le X\le b} = \int_a^b f(x) dx</script><p>The function has to be $f(x) \ge 0$ for all $x$ and $\int_{-\infty}^\infty f(x) dx = 1$. We call $f$ the <strong>probability density function (pdf)</strong> of $X$.</p>
<p>​    Note: it is important that we don’t define $f(x)$ without <script type="math/tex">\P{X}</script>, in other words, the pmf of $X$ is defined by <script type="math/tex">p_X(x) = \P{X=x}</script>, but $f(x) = \frac{dP(X)}{dx}|_x$, we couldn’t find $f(x)$ without find <script type="math/tex">\P{X}</script> firstly. (Right, we have some tricks or skills to solve pdf, but the definition is should follow these steps).</p>
<h4 id="3-4-Cumulative-Distribution-Functions"><a href="#3-4-Cumulative-Distribution-Functions" class="headerlink" title="3.4 Cumulative Distribution Functions."></a>3.4 Cumulative Distribution Functions.</h4><h6 id="Definition-3-9-Cumulative-Distribution-Function"><a href="#Definition-3-9-Cumulative-Distribution-Function" class="headerlink" title="Definition 3.9 (Cumulative Distribution Function)"></a>Definition 3.9 (Cumulative Distribution Function)</h6><p>The <strong>cumulative distribution function (cdf)</strong> $F$ of a random variable $X$ is the function $F:\mathbb{R} \to [0,1]$ defined by </p>
<script type="math/tex; mode=display">
F(a) = \P{X\le a}\ \ \ \text{ for }\ -\infty<x<\infty</script><p>​    Both pmf (pdf) and cdf of a random variable contain all probabilistic information of $X$. In particular, if X is a discrete random variable supported on $\{d_1,\cdots,d_m\}$, its cdf can be obtained by </p>
<script type="math/tex; mode=display">
F(a) = \sum_{d_i\le a} p_X(d_i)</script><h6 id="Proposition-1"><a href="#Proposition-1" class="headerlink" title="Proposition 1."></a>Proposition 1.</h6><p>The cdf $F(x)$ of $X$ satisfies the following three properties.</p>
<ul>
<li><p>For $a\le b$, one has $F(a) \le F(b)$.</p>
</li>
<li><p>Since $F(a)$ is a probability, we have </p>
<script type="math/tex; mode=display">
\lim\limits_{x\to-\infty}F(x) = 0\ \ \ \ \ \lim\limits_{x\to\infty}F(x) = 1</script></li>
<li><p>The cdf <script type="math/tex">F(x) = \P{X\le x}</script>, it is always continuous from the right.</p>
</li>
</ul>
<h6 id="Theorem-3-2"><a href="#Theorem-3-2" class="headerlink" title="Theorem 3.2."></a>Theorem 3.2.</h6><p>Let $X$ be a continuous random variable, and let $f(x)$ and $F(x)$ denote its pdf and cdf, respectively. Then for every real $x$</p>
<script type="math/tex; mode=display">
F(x) = \int_{-\infty}^x f(t)\ dt</script><p>and</p>
<script type="math/tex; mode=display">
\frac{d}{dx} F(x) = f(x)</script><p>at all $x$ where $f(x)$ is continuous.</p>
<h6 id="Proposition-2-More-Properties"><a href="#Proposition-2-More-Properties" class="headerlink" title="Proposition 2 (More Properties)."></a>Proposition 2 (More Properties).</h6><p>For real numbers $x$ and $a&lt;b$, we have</p>
<ul>
<li><script type="math/tex; mode=display">\P{X>x} = 1-F(x)</script></li>
<li><script type="math/tex; mode=display">\P{a<X\le b} = F(b) - F(a)</script></li>
<li><script type="math/tex; mode=display">\P{X<x} = \lim_{t\to x^-} F(t)$$ and $$\P{X=x} = F(x) - \lim_{t\to x^-}F(t)</script></li>
</ul>
<h6 id="Definition-3-10"><a href="#Definition-3-10" class="headerlink" title="Definition 3.10."></a>Definition 3.10.</h6><p>Let $X$ be a random variable and let $p \in(0,1)$. The $p$ <strong>quantile</strong> or <strong>100$p$th percentile</strong> of the distribution of $X$ is the the smallest number $q_p$ s.t.</p>
<script type="math/tex; mode=display">
F(q_p) = \P{X\le q_p} \ge p</script><p>The <strong>median</strong> of a distribution is its $50th$ percentile.</p>
<p>​    I don’t think this is useful…</p>
<h4 id="3-5-Joint-Distribution"><a href="#3-5-Joint-Distribution" class="headerlink" title="3.5. Joint Distribution"></a>3.5. Joint Distribution</h4><h6 id="Definition-3-11-Discrete-Joint-Distributions"><a href="#Definition-3-11-Discrete-Joint-Distributions" class="headerlink" title="Definition 3.11 (Discrete Joint Distributions)."></a>Definition 3.11 (Discrete Joint Distributions).</h6><p> The <strong>joint probability mass function</strong> $p_{X,Y}$ of two discrete random variables $X$ and $Y$ is the function $p_{X,Y}:\mathbb{R}^2\to[0,1]$, define by </p>
<script type="math/tex; mode=display">
p_{X,Y}(a,b) = \P{X=a, Y=b}\hspace{1cm}\text{for}-\infty<a,b<\infty</script><h6 id="Theorem-3-3"><a href="#Theorem-3-3" class="headerlink" title="Theorem 3.3."></a>Theorem 3.3.</h6><p>Let $X$ and $Y$ have a discrete joint distribution. If $(x,y)$ is not one of the possible values of the pair $(X,Y)$, then $p_{X,Y}(x,y)=0$. Moreover, </p>
<script type="math/tex; mode=display">
\sum_{(x,y)\in\mathbb{R}^2}p_{X,Y}(x,y) =1</script><p>For each set $C \subset \mathbb{R}^2$, </p>
<script type="math/tex; mode=display">
\P{(X,Y)\in C} = \sum_{(x,y)\in C} p_{X,Y}(x,y)</script><h6 id="Definition-3-12-Continuous-Joint-Distributions"><a href="#Definition-3-12-Continuous-Joint-Distributions" class="headerlink" title="Definition 3.12 (Continuous Joint Distributions)."></a>Definition 3.12 (Continuous Joint Distributions).</h6><p>Random variables $X$ and $Y$ have a <strong>joint continuous distribution</strong> if for some function $f_{X,Y}:\mathbb{R}^2 \to \mathbb{R}$ and for all numbers $a_1, a_2$ and $b_1,b_2$ with $a_1\le b_1$ and $a_2\le b_2$, </p>
<script type="math/tex; mode=display">
\P{a_1\le X\le b_1, a_2\le Y\le b_2} = \int_{a_1}^{b_1}\int_{a_2}^{b_2} f_{X,Y}(x,y)dxdy</script><p>The function $f_{X,Y}$ has to be nonnegative for all $x$, $y$, and $\int_{-\infty}^\infty \int_{-\infty}^\infty f_{X,Y}(x,y)dxdy = 1$. We call $f_{X,Y}$ the <strong>joint probability density function</strong> of $X$ and $Y$.</p>
<p>​    Note: I will omit the subscript $X,Y$ if there is no ambiguity.</p>
<h6 id="Theorem-3-4"><a href="#Theorem-3-4" class="headerlink" title="Theorem 3.4."></a>Theorem 3.4.</h6><p>For every continuous joint distribution, the following statements hold:</p>
<ul>
<li>Every individual point, and every infinite sequence of points, in the $xy$-plane has probability 0.</li>
<li><p>Let $f$ be one real variable  continuous function defined on $(a,b)$. The sets $\{(x,y):y=f(x), x\in(a,b)\}$ and $\{(x,y):x=f(y), y\in(a,b)\}$ have probability 0.</p>
<p>Note that if $X$ is a continuous random variable and $Y=X$, then <strong>both $X$ and $Y$ are continuous</strong> but $(X,Y)$ lies on a straight line $y=x$ in $\mathbb{R}^2$. Hence, by Theorem 3.4, $X$ and $Y$ <strong>cannot have a continuous joint distribution</strong>.</p>
</li>
</ul>
<p>​    In this part, we always could choose a part $C \subset \mathbb{R}^2$ to make <script type="math/tex">\P{(X,Y)\in C} <0</script>, which could proves this Theorem.</p>
<h6 id="Definition-3-13-Mixed-Joint-Distribution"><a href="#Definition-3-13-Mixed-Joint-Distribution" class="headerlink" title="Definition 3.13 (Mixed Joint Distribution)."></a>Definition 3.13 (Mixed Joint Distribution).</h6><p>Let $X$ be a discrete random variable and $Y$ is continuous, They have a <strong>mixed joint distribution</strong> if there is a function $f$ such that </p>
<script type="math/tex; mode=display">
\P{X\in A\ \text{and }\ Y\in B} = \int_{B}\sum_{x\in A} f(x,y) dy</script><p>for all pair $A, B\subset \mathbb R$ . The function $f(x,y)$ has to be nonnegative and $\int_{-\infty}^\infty \sum_{-\infty}^\infty f(x,y)dy=1$, We call the <strong>mixed joint probability function</strong> of $X$ and $Y$.</p>
<h4 id="3-6-Marginal-Distributions"><a href="#3-6-Marginal-Distributions" class="headerlink" title="3.6. Marginal Distributions"></a>3.6. Marginal Distributions</h4><h6 id="Definition-3-14"><a href="#Definition-3-14" class="headerlink" title="Definition 3.14."></a>Definition 3.14.</h6><p>The <strong>joint cumulative distribution function </strong> of two random variables $X$ and $Y$ is define as </p>
<script type="math/tex; mode=display">
F_{X,Y}(x,y) = \P{X\le x\ \text{and}\ Y\le y}</script><h6 id="Definition-3-15-Marginal-Distribution"><a href="#Definition-3-15-Marginal-Distribution" class="headerlink" title="Definition 3.15 (Marginal Distribution)."></a>Definition 3.15 (Marginal Distribution).</h6><p>Suppose $F_{X,Y}(x,y)$ is the joint cdf for random variables $X$ and $Y$, The <strong>marginal cdf</strong> of $X$ is </p>
<script type="math/tex; mode=display">
F_X(x) = \lim_{y\to\infty} F_{X,Y}(x,y)</script><p>Symmetrically, we can define marginal cdf of $Y$ by taking limit of $F_{X,Y}$ as $x$ goes to infinity.</p>
<p>​    The pmf(pdf) associated with the joint marginal cdf of $X$ is called the <strong>marginal pmf(pdf)</strong> of $X$.</p>
<h6 id="Theorem-3-5"><a href="#Theorem-3-5" class="headerlink" title="Theorem 3.5."></a>Theorem 3.5.</h6><p>Suppose $X$, $Y$ are random variables with a joint pf $f_{X,Y}(x,y)$. Then the marginal pmf (pdf) of $X$ can be obtained by </p>
<script type="math/tex; mode=display">
f_X(x) = \sum_{y=-\infty}^\infty f_{X,Y}(x,y)</script><p>or </p>
<script type="math/tex; mode=display">
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y)\ dy</script><p>depending on whether $Y$ is discrete or continuous.</p>
<h6 id="Definition-3-16-Independent-Random-Variables"><a href="#Definition-3-16-Independent-Random-Variables" class="headerlink" title="Definition 3.16  (Independent Random Variables)"></a>Definition 3.16  (Independent Random Variables)</h6><p>Two random variables $X$ and $Y$ are said to be independent if for all events $A, B\subset \mathbb{R}$, </p>
<script type="math/tex; mode=display">
\P{X\in A\ \text{and}\ Y\in B} = \P{X\in A} \P{Y\in B}.</script><p>Random variables that are <strong>not independent</strong> are called <strong>dependent</strong>.</p>
<p>​    It follows immediately  from the definition that if $X$ and $Y$ are independent, then</p>
<script type="math/tex; mode=display">
\P{X\le x\ \text{and}\ Y\le y} =\P{X\le x}\P{Y\le y}</script><h6 id="Theorem-3-6-Equivalent-Definition-of-Independency"><a href="#Theorem-3-6-Equivalent-Definition-of-Independency" class="headerlink" title="Theorem 3.6 (Equivalent Definition of Independency)."></a>Theorem 3.6 (Equivalent Definition of Independency).</h6><p>Let $F(x,y)$ be the joint cdf of $X$ and $Y$, Let $F_X(x)$ and $F_Y(y)$ be the marginal cdf for $X$ and $Y$ repectively. Then $X$ and $Y$ are independent if and only if </p>
<script type="math/tex; mode=display">
F(a,b) = F_X(a)F_Y(b)</script><p>for all real values $a, b$</p>
<p>​    As a special case we can take $A=\{a\}$ and $B = \{b\}$, then </p>
<script type="math/tex; mode=display">
\P{X=a\ \text{and}\ Y=b} = \P{X=a}\P{Y=b}</script><h6 id="Theorem-3-7"><a href="#Theorem-3-7" class="headerlink" title="Theorem 3.7."></a>Theorem 3.7.</h6><p>Suppose $f(x,y)$ is the joint pmf (pdf) of $X$ and $Y$. Then $X$ and $Y$ are independent if and only if </p>
<script type="math/tex; mode=display">
f(x,y) = f_X(x)f_Y(y)</script><h4 id="3-7-Conditional-Distributions"><a href="#3-7-Conditional-Distributions" class="headerlink" title="3.7. Conditional Distributions"></a>3.7. Conditional Distributions</h4><h6 id="Definition-3-17"><a href="#Definition-3-17" class="headerlink" title="Definition 3.17."></a>Definition 3.17.</h6><p>Let $X$ and $Y$ be random variables with joint pmf (pdf) $f(x,y)$. Let $f_Y$ denote the marginal pmf (pdf) of $Y$ such that for all $f_Y(y)&gt;0$ for all $y$. Define </p>
<script type="math/tex; mode=display">
g_X(x|y) = \frac{f(x,y)}{f_Y(y)}</script><p>Then $g_X(\cdot|y)$ is called the <strong>conditional pmf(pdf)</strong> of $X$ given $Y$. The distribution whose pmf(pdf) is $g_X(\cdot|y)$ is called the <strong>conditional distribution</strong> of $X$ given $Y=y$. </p>
<h4 id="3-8-Multivariate-Distributions"><a href="#3-8-Multivariate-Distributions" class="headerlink" title="3.8. Multivariate Distributions."></a>3.8. Multivariate Distributions.</h4><p>In this section, we shall extend the results for two random variables to an arbitrary finite number $n$ of random variables $X_1,X_2,\cdots,X_n$. In general, the joint distribution of more than two random variables is called a multivariate distribution.</p>
<h6 id="Definition-3-18"><a href="#Definition-3-18" class="headerlink" title="Definition 3.18."></a>Definition 3.18.</h6><p>The <strong>joint cumulative distribution function</strong> of $X_1,\cdots,X_n$, which is defined by </p>
<script type="math/tex; mode=display">
F(a_1,a_2,\cdots,a_n) = \P{X_1\le a_1, \cdots, X_n\le a_n}</script><p>for $-\infty &lt;a_1,\cdots,a_n&lt;\infty$. </p>
<p>If the random variables is discrete, the joint distribution can also be characterized by specifying the joint probability mass function $p$ of $X_1,X_2,\cdots,X_n$, defined by </p>
<script type="math/tex; mode=display">
p(a_1,a_2,\cdots,a_n) = \P{X_1=a_1,\cdots,X_n=a_n}</script><p>for $-\infty &lt; a_1,\cdots,a_n&lt;\infty$ .</p>
<h6 id="Theorem-3-8"><a href="#Theorem-3-8" class="headerlink" title="Theorem 3.8."></a>Theorem 3.8.</h6><p>If $\mathbf{X} = (X_i)_{i=1}^n$ has a joint discrete distribution with pmf $f$, then for every subset $C\subset R^n$, </p>
<script type="math/tex; mode=display">
\P{\bold{X}\in C} = \sum_{\bold{x}\in C}f(\bold{x})</script><h6 id="Definition-3-19"><a href="#Definition-3-19" class="headerlink" title="Definition 3.19."></a>Definition 3.19.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables. The <strong>joint continuous distribution function</strong> is a nonnegative function $f:\mathbb{R}^n \to \mathbb{R}$ such that for all $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ </p>
<script type="math/tex; mode=display">
\P{\cap_{i=1}^n \{a_i\le X_i\le b_i\}} = \int_{a_1}^{b_1}\cdots\int_{a_n}^{b_n} f(x_1,\cdots,x_n)\ dx_1\cdots dx_n</script><p>Here $f$ satisfy $\int f = 1$. We call $f$ the <strong>joint probability density function </strong> of $\mathbf{X}$.</p>
<h6 id="Theorem-3-9"><a href="#Theorem-3-9" class="headerlink" title="Theorem 3.9."></a>Theorem 3.9.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables. The joint pdf $f$ can derived from the joint cdf $F$ by </p>
<script type="math/tex; mode=display">
f(x_1,\cdots,x_n)= \frac{\partial^n}{\partial x_1\cdots\partial x_n}F(x_1,\cdots,x_n)</script><p>​    Similar to the two variable case, the marginal cdf $F_j(x_j)$ of $X_j$ can be obtained by </p>
<script type="math/tex; mode=display">
F_j(x_j) = \P{X_j\le x_j}=\lim\limits_{\substack{x_i\to\infty\\i\neq j}} F(x_1,\cdots,x_n)</script><h6 id="Definition-3-20"><a href="#Definition-3-20" class="headerlink" title="Definition 3.20."></a>Definition 3.20.</h6><p>Random variables $(X_i)_{i=1}^n$ are <strong>independent</strong> if for every $n$ sets $A_1,\cdots,A_n$ of real numbers.</p>
<script type="math/tex; mode=display">
\P{\cap_{i=1}^n\{X_i\subset A_i\}} = \prod_{i=1}^n \P{X_i\subset A_i}.</script><h6 id="Definition-3-21-i-i-d"><a href="#Definition-3-21-i-i-d" class="headerlink" title="Definition 3.21 (i.i.d.)."></a>Definition 3.21 (i.i.d.).</h6><p> A collection of random variables $(X_i)_{i=1}^n$ is called <strong>independent and identically distributed</strong> if each Xi has the same probability distribution and they are mutually independent. This property is often abbreviated as <strong>i.i.d.</strong></p>
<h4 id="3-9-Function-of-One-Random-Variable"><a href="#3-9-Function-of-One-Random-Variable" class="headerlink" title="3.9. Function of One Random Variable."></a>3.9. Function of One Random Variable.</h4><h6 id="Theorem-3-10"><a href="#Theorem-3-10" class="headerlink" title="Theorem 3.10."></a>Theorem 3.10.</h6><p>Let $X$ be a discrete random variable with pmf $p_X(x)$ and let $Y = r(X)$ for some function $r$ defined on the support of $X$. Then the pmf of $Y$ is</p>
<script type="math/tex; mode=display">
p_Y(y) = \P{Y=y} = \P{r(X) = y} =\sum_{r(x)=y} p_X(x)</script><p>​    If $r$ is invertible on the support of $X$, the above theorem can be simplified to </p>
<script type="math/tex; mode=display">
p_Y(y) = \sum_{x=r^{-1}(y)}p_X(x)</script><h6 id="Theorem-3-11-Change-of-Units-Transformation"><a href="#Theorem-3-11-Change-of-Units-Transformation" class="headerlink" title="Theorem 3.11 (Change-of-Units Transformation)."></a>Theorem 3.11 (Change-of-Units Transformation).</h6><p>Suppose $X$ is continuous random variable with pdf $f_X(x)$. If we make a linear variable change $Y=g(X) =aX+b$ for $a\neq 0$, the pdf of $Y$ is </p>
<script type="math/tex; mode=display">
f_Y(y) = \frac1{|a|}f_X(\frac{y-b}a)</script><p>for $y\in\mathbb{R}$.</p>
<p>$Proof.$ By cdf. </p>
<h6 id="Theorem-3-12-Probability-Integral-Transformation"><a href="#Theorem-3-12-Probability-Integral-Transformation" class="headerlink" title="Theorem 3.12 (Probability Integral Transformation)"></a>Theorem 3.12 (Probability Integral Transformation)</h6><p>Let $X$ have a continuous cdf $F$ and let $Y=F(X)$. The distribution of $Y$ is the uniform distribution on the interval $[0,1]$ </p>
<p>$Proof$. It equals to $F(y)=y$ for $0\leq y\leq 1$. Let $q_y$ be the $y$ quantile point of $F$. Since $F$ is continuous, $F(q_y)=y$. Then by the property the quantile function, $X\le q_y$ iff $F(X)\le y$.</p>
<script type="math/tex; mode=display">
F(y) = \P{Y\le y} = \P{X\le q_y} = F(q_y)=y</script><p>Which proves the theorem.</p>
<h6 id="Theorem-3-13"><a href="#Theorem-3-13" class="headerlink" title="Theorem 3.13."></a>Theorem 3.13.</h6><p>too trival</p>
<h4 id="3-10-Functions-of-Two-Random-Variables"><a href="#3-10-Functions-of-Two-Random-Variables" class="headerlink" title="3.10. Functions of Two Random Variables."></a>3.10. Functions of Two Random Variables.</h4><p>Suppose $(X_i)_{i=1}^n$ are random variables with a discrete joint pmf $p_X$. Let </p>
<script type="math/tex; mode=display">
Y_i = r_i(X_1,\cdots,X_n)\hspace{1cm}\text{for }\ 1\le i\le m</script><p><img src="C:\Users\85125\AppData\Roaming\Typora\typora-user-images\image-20211204133102452.png" alt="image-20211204133102452"></p>
<h6 id="Theorem-3-17"><a href="#Theorem-3-17" class="headerlink" title="Theorem 3.17."></a>Theorem 3.17.</h6><p>Let $f_{X_1,X_2}(x_1,x_2)$ be the joint pdf of $X_1$, $X_2$ and let $Y=a_1X_1+a_2X_2+b$ with $a_1\neq 0$, the pdf of $Y$ is </p>
<script type="math/tex; mode=display">
f_Y(y) = \int_{-\infty}^\infty f_{X_1,X_2}(\frac{y-b-a_2x_2}{a_1},x_2)\frac1{|a_1|}dx_2</script><h6 id="Definition-3-22-Convolution"><a href="#Definition-3-22-Convolution" class="headerlink" title="Definition 3.22 (Convolution)."></a>Definition 3.22 (Convolution).</h6><p>Let $X_1, X_2$ be independent continuous random variables and let $Y = X_1 + X_2$. The distribution of Y is called the <strong>convolution</strong> of the distributions of $X_1$ and $X_2$.</p>
<p>​    If $X_1$ and $X_2$ are discrete random variables, then </p>
<script type="math/tex; mode=display">
p_Y(y) =\sum_{x=-\infty}^\infty p_1(x)p_2(y-x)</script><p>​    For continuous random variables $X_1,X_2$ with pdf $f_1,f_2$, then </p>
<script type="math/tex; mode=display">
f(y) = \int_{-\infty}^\infty f_1(x)f_2(y-x)\ dx</script><h6 id="More-transform"><a href="#More-transform" class="headerlink" title="More transform"></a>More transform</h6><p>​    Suppose $X$ and $Y$ is continuous random variables and let $Z=XY$, then the pdf of $Z$ is  </p>
<script type="math/tex; mode=display">
f_Z(z) = \int_{-\infty}^\infty \frac1{|x|}f_{X,Y}(x,\frac{z}{x})\ dx</script><p>​    If we let $Z=\frac{X}{Y}$, the pdf of $Z$ is </p>
<script type="math/tex; mode=display">
f_Z(z) = \int_{-\infty}^\infty f_{X,Y}(zy,y)|y|\ dy</script><h6 id="Theorem-3-18"><a href="#Theorem-3-18" class="headerlink" title="Theorem 3.18."></a>Theorem 3.18.</h6><p>Let $(X_i)_{i=1}^n$ be continuous random variables with joint pdf  $f_X$ and supported on $\mathcal{S} \subset \mathbb{R}^n$, Define </p>
<script type="math/tex; mode=display">
Y_j = r_j(\mathbf{X})</script><p>for $j=1,\cdots,n$. Here, we assume that $r_j$ define a one-to-one differentiable transformation of $\mathcal{S}$ onto a subset $\mathcal{T}$ of $\mathbb{R}^n$. Let this inverse of this transformation be given as </p>
<script type="math/tex; mode=display">
x_i = s_i(\mathbf{y})</script><p>for $i=1,\cdots,n$. Then the joint pdf of $\mathbf{Y}$ is </p>
<script type="math/tex; mode=display">
f_{\mathbf{Y}}(\mathbf{y}) = \cases{f_{\mathbf{X}}(s)|J|\hspace{1cm}&for $\mathbf{y}\in\mathcal{T}$ \\0&otherwise}</script><p>where $J$ is the Jacobian determinant </p>
<script type="math/tex; mode=display">
J = \det\begin{bmatrix}
\frac{\partial s_1}{\partial y_1}&\cdots&\frac{\partial s_1}{\partial y_n}\\
\vdots&&\vdots\\
\frac{\partial s_n}{\partial y_1}&\cdots&\frac{\partial s_n}{\partial y_n}
\end{bmatrix}</script><p>​    Note: The condition $r_j$ is defined a one-to-one differentiable transformation is important. If the $r_j$ isn’t one-to-one, it is always wrong because we ignore some information in the transformation.</p>
<h4 id="3-11-Markov-Chains"><a href="#3-11-Markov-Chains" class="headerlink" title="3.11. Markov Chains."></a>3.11. Markov Chains.</h4><p>​    This section will not appear in my test. And the difficult part is how to construct the state instead of calculate.</p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>捏马这一章的东西实在是太多了. 打了好几天才打完…</p>
<p>首先我们找到了一个map使得sample space上的event可以被映射到$\mathbb{R}$上, 定义这个map叫做 random variable. 在定义了random variable之后我们就可以在$\mathbb{R}$上研究概率, 如此便可以定义一系列的cdf,pdf,以及joint cdf, pdf, marginal cdf, pdf. 另一方面, 类似事件独立我们可以定义随机变量的独立, 这种独立关系又可以等价到pdf与joint pdf的关系上(这是比较显然的).</p>
<p>在此之后, 对于随机变量的组合变换, 我们可以利用cdf $\Rightarrow$ 概率 $\Rightarrow$ cdf‘ $\Rightarrow$ pdf’ 推导单一随机变量的新的cdf，对于联合分布可以使用雅可比行列式来处理, 本质都是从某个support上的某个微元映射到其他support上, 这个结果虽然不好证明但是非常自然的.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/PS-chapter-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/PS-chapter-2/" class="post-title-link" itemprop="url">PS chapter 2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-27 14:18:23" itemprop="dateCreated datePublished" datetime="2021-11-27T14:18:23+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-05 14:10:00" itemprop="dateModified" datetime="2021-12-05T14:10:00+08:00">2021-12-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-and-Statistics/" itemprop="url" rel="index"><span itemprop="name">Probability and Statistics</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="2-Conditional-Probability"><a href="#2-Conditional-Probability" class="headerlink" title="2. Conditional Probability"></a>2. Conditional Probability</h3><h4 id="2-1-The-Definition-of-Condition-Probability"><a href="#2-1-The-Definition-of-Condition-Probability" class="headerlink" title="2.1 The Definition of Condition Probability"></a>2.1 The Definition of Condition Probability</h4><p> A major use of probability in statistical inference is the updating of probability when certain events are observed. The updated probability of event $A$ after we learn that event $B$ has occurred is the conditional probability of $A$ given $B$.</p>
<h6 id="Definition-2-1"><a href="#Definition-2-1" class="headerlink" title="Definition 2.1."></a>Definition 2.1.</h6><p>The <strong>condition probability</strong> of $A$ given $B$ is defined as </p>
<script type="math/tex; mode=display">
\P{A|B} = \frac{\P{A\cap B}}{\P{B}}</script><p> provided <script type="math/tex">\P{B} >0</script>.</p>
<p>​    And the conditional probability <script type="math/tex">\P{A|B}</script> is not defined if <script type="math/tex">\P{B}=0</script>.</p>
<h6 id="Theorem-2-1-Multiplication-Rule"><a href="#Theorem-2-1-Multiplication-Rule" class="headerlink" title="Theorem 2.1 (Multiplication Rule)."></a>Theorem 2.1 (Multiplication Rule).</h6><p>Let $A$, $B$ be events. If <script type="math/tex">\P{B}>0</script>, then </p>
<script type="math/tex; mode=display">
\P{A\cap B} =\P{A|B}\P{B}</script><h6 id="Theorem-2-2"><a href="#Theorem-2-2" class="headerlink" title="Theorem 2.2."></a>Theorem 2.2.</h6><p>Suppose that $A_1,A_2,\cdots,A_n$ are events such that <script type="math/tex">\P{A_1\cap A_2\cap\cdots\cap A_{n-1}} >0</script>. Then </p>
<script type="math/tex; mode=display">
\P{A_1\cap A_2 \cap\cdots\cap A_n} = \P{A_1}\P{A_2|A_1}\cdots\P{A_n|A_1\cap A_2\cap\cdots\cap A_{n-1}}</script><h6 id="Theorem-2-3"><a href="#Theorem-2-3" class="headerlink" title="Theorem 2.3."></a>Theorem 2.3.</h6><p>Suppose that $A_1,A_2,\cdots A_n$, $B$ are events such that <script type="math/tex">\P{B}>0</script> and <script type="math/tex">\P{A_1\cap A_2\cap\cdots \cap A_{n-1}|B}>0</script>. Then </p>
<script type="math/tex; mode=display">
\begin{align*}
\P{A_1\cap A_2 \cap\cdots\cap A_n|B} =& \P{A_1|B}\P{A_2|A_1\cap B} \cdots\\
&\times\P{A_n|A_1\cap A_2\cap\cdots\cap A_{n-1}\cap B}
\end{align*}</script><h6 id="Definition-2-2"><a href="#Definition-2-2" class="headerlink" title="Definition 2.2."></a>Definition 2.2.</h6><p>Let $\Omega$ be a sample space and let $B_1, \cdots B_k$ be $k$ events in $\Omega$. If $B_1,\cdots,B_k$ are disjoint and $\cup_{j=1}^k B_j = \Omega$, then these events form a <strong>partition</strong> of $\Omega$. </p>
<h6 id="Theorem-2-4-Low-of-total-probability"><a href="#Theorem-2-4-Low-of-total-probability" class="headerlink" title="Theorem 2.4 (Low of total probability)."></a>Theorem 2.4 (Low of total probability).</h6><p>Suppose that $B_1,\cdots,B_k$ form a partition of the space $\Omega$ and <script type="math/tex">\P{B_j}>0</script> for all $j$. Then for every event $A \subset \Omega$, </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j}\P{A|B_j}</script><p>$Proof.$ We can rewrite $A$ as </p>
<script type="math/tex; mode=display">
A = (B_1 \cap A) \cup (B_2\cap A) \cup\cdots \cup(B_k \cap A)</script><p>Thus </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j\cap A} = \sum_{j=1}^k \P{B_j}\P{A|B_j}</script><h6 id="Theorem-2-5"><a href="#Theorem-2-5" class="headerlink" title="Theorem 2.5."></a>Theorem 2.5.</h6><p>The law of total probability has an analog conditional on another event $C$ </p>
<script type="math/tex; mode=display">
\P{A|C} = \sum_{j=1}^k \P{B_j|C}\P{A|B_j\cap C}</script><p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{B_j|C}\P{A|B_j\cap C} = \frac{\P{B_j \cap C}\P{A\cap B_j \cap C}}{\P{C} \P{B_j\cap C}} = \P{A\cap B_j|C}</script><p>Thus </p>
<script type="math/tex; mode=display">
\sum_{j=1}^k \P{B_j|C}\P{A|B_j\cap C} = \sum_{j=1}^k \frac{\P{A\cap C\cap B_j}}{\P{C}} = \frac{\P{A\cap C}}{\P{C}} = \P{A|C}</script><h4 id="2-2-Independent-Events"><a href="#2-2-Independent-Events" class="headerlink" title="2.2 Independent Events."></a>2.2 Independent Events.</h4><p>If learning that $B$ has occurred does not change the probability of $A$, then we say that $A$ and $B$ are independent.</p>
<h6 id="Definition-2-3"><a href="#Definition-2-3" class="headerlink" title="Definition 2.3."></a>Definition 2.3.</h6><p>Two events $A$ and $B$ are <strong>independent</strong> if </p>
<script type="math/tex; mode=display">
\P{A\cap B} = \P{A}\P{B}</script><h6 id="Theorem-2-6"><a href="#Theorem-2-6" class="headerlink" title="Theorem 2.6."></a>Theorem 2.6.</h6><p>Suppose <script type="math/tex">\P{B}>0</script> or <script type="math/tex">\P{A}>0</script>. Then $A$ and $B$ are independent if and only if <script type="math/tex">\P{A|B} = \P{A}</script> or <script type="math/tex">\P{B|A} = \P{B}</script>, respectively.</p>
<h6 id="Theorem-2-7"><a href="#Theorem-2-7" class="headerlink" title="Theorem 2.7."></a>Theorem 2.7.</h6><p>If $A$ and $B$ are independent, then $A$ and $B^c$ are also independent.</p>
<p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{A\cap B^c} = \P{A} - \P{A\cap B} = \P{A}(1-\P{B}) = \P{A}\P{B^c}</script><h6 id="Definition-2-4-Mutually-independent"><a href="#Definition-2-4-Mutually-independent" class="headerlink" title="Definition 2.4 (Mutually independent)."></a>Definition 2.4 (Mutually independent).</h6><p>The $k$ events $A_1,\cdots,A_k$ are <strong>mutually independent</strong> if for any subset <script type="math/tex">\mathcal{M} \subset \{1,\cdots,k\}</script>, </p>
<script type="math/tex; mode=display">
\P{\bigcap_{j \in \mathcal{M}}A_j} = \prod_{j \in \mathcal{M}} \P{A_j}</script><p>​    Note: The arbitrariness of $\mathcal{M}$ is very important for mutually independent. We can give a conquer  example to show that although $A_j$ are <strong>pairwise independent</strong>, $A_j$ are not mutually independent.</p>
<h6 id="Theorem-2-8"><a href="#Theorem-2-8" class="headerlink" title="Theorem 2.8."></a>Theorem 2.8.</h6><p>Let $A_i (i=1,\cdots,k)$ be events that <script type="math/tex">\P{\cap_{i=1}^k A_i}>0</script>. Then $A_i$ are independent if and only if for every two joint subsets $\mathcal{I},\mathcal{J}\subset\{1,\cdots,k\}$, $\mathcal{I}\cap\mathcal{J}=\varnothing$, we have </p>
<script type="math/tex; mode=display">
\P{\bigcap_{i\in\mathcal{I}}A_i|\bigcap_{j\in\mathcal{J}}A_j} = \P{\bigcap_{i\in\mathcal{I}} A_i}.</script><p>​    This theorem says that $k$ events are independent if and only if learning that some of the events occur does not change the probability that any combination of the other event occur.</p>
<h6 id="Definition-2-5"><a href="#Definition-2-5" class="headerlink" title="Definition 2.5."></a>Definition 2.5.</h6><p>We say that event $A_i (i=1,\cdots,k)$ are <strong>conditionally independent</strong> given $B$ if for every subcollection $\mathcal{I} \subset\{1,\cdots,k\}$, </p>
<script type="math/tex; mode=display">
\P{\bigcap_{i\in\mathcal{I}} A_i|B} = \prod_{i\in\mathcal{I}} \P{A_i|B}</script><p>​    Note: The definition is identical to that of independent events with modification that all probabilities are now conditional on B. It is important that even if we assume $A_i$ are conditionally independent given $B$, it is <strong>not</strong> necessary that $A_i$ are conditionally independent given $B^c$ </p>
<h6 id="Theorem-2-10"><a href="#Theorem-2-10" class="headerlink" title="Theorem 2.10."></a>Theorem 2.10.</h6><p>Suppose that $A_1, A_2$ and $B$ are events such that <script type="math/tex">\P{A_1\cap B}>0</script>. Then $A_1$ and $A_2$ are conditionally independent given $B$ if and only if <script type="math/tex">\P{A_2|A_1\cap B} = \P{A_2|B}</script></p>
<p>$Proof.$ If $A_1$ and $A_2$ are independent </p>
<script type="math/tex; mode=display">
\P{A_1\cap A_2|B} = \P{A_1|B} \P{A_2|B}</script><p>consider that  <script type="math/tex">\P{A_1\cap A_2 |B} = \P{A_1\cap B}\P{A_2|A_1\cap B}</script>. Comparing could proves the theorem.</p>
<h4 id="2-3-Bayes’-Theorem"><a href="#2-3-Bayes’-Theorem" class="headerlink" title="2.3. Bayes’ Theorem"></a>2.3. Bayes’ Theorem</h4><h6 id="Theorem-2-11-Bayes’-Theorem"><a href="#Theorem-2-11-Bayes’-Theorem" class="headerlink" title="Theorem 2.11 (Bayes’ Theorem)."></a>Theorem 2.11 (Bayes’ Theorem).</h6><p>Let $B_i$ for $i=1,\cdots,k$ form a partition of the space $\Omega$ and <script type="math/tex">\P{B_i}>0</script> for all $i$. Let $A$ be an event such that <script type="math/tex">\P{A}>0</script>, then for all $i$:</p>
<script type="math/tex; mode=display">
\P{B_i|A} = \frac{\P{B_i}\P{A|B_i}}{\sum_{j=1}^k\P{B_j}\P{A|B_j}}</script><p>$Proof.$ Consider </p>
<script type="math/tex; mode=display">
\P{B_i|A} = \frac{\P{B_i \cap A}}{\P{A}} = \frac{\P{B_i}\P{A|B_i}}{\P{A}}</script><p>and </p>
<script type="math/tex; mode=display">
\P{A} = \sum_{j=1}^k \P{B_j} \P{A|B_j}</script><p>Which proves the theorem.</p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>这一章围绕着条件概率展开, 主要研究的是如果某些events被观测到后, 其余event的概率会发生怎样的变化. 接下来就按照是否会发生影响来定义了事件的相互独立, 以及类似地 mutually independent 和 conditionally independent (这两个是对一个序列的事件而言的). 以及给出了一个很常用的定理Bayes’ Theorem, 这个定理给出了一个<strong>prior probability</strong>与<strong>posterior probability</strong>的关系, 因此也会带来很多反直觉的结果. 应用这个定理时最重要的时要确定每一步的过程的条件究竟是什么样的, 否则必然带来一些奇怪的错误. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/PS%20chapter%201/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/PS%20chapter%201/" class="post-title-link" itemprop="url">PS chapter 1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-27 11:00:58" itemprop="dateCreated datePublished" datetime="2021-11-27T11:00:58+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-05 14:08:39" itemprop="dateModified" datetime="2021-12-05T14:08:39+08:00">2021-12-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Probability-and-Statistics/" itemprop="url" rel="index"><span itemprop="name">Probability and Statistics</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="math/tex; mode=display">
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}t} \left(#1\right)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\renewcommand{\P}[1]{\mathbf{P}(#1)}
% \newcommand{\E}[1]{\mathbf{E}(#1)}
\newcommand{\E}[1]{\mathrm{E}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\t}{\times}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}</script><h3 id="1-Introduce-to-Probability"><a href="#1-Introduce-to-Probability" class="headerlink" title="1. Introduce to Probability"></a>1. Introduce to Probability</h3><h4 id="1-1-Events-and-Sample-Spaces"><a href="#1-1-Events-and-Sample-Spaces" class="headerlink" title="1.1 Events and Sample Spaces."></a>1.1 Events and Sample Spaces.</h4><h6 id="Definition-1-1"><a href="#Definition-1-1" class="headerlink" title="Definition 1.1."></a>Definition 1.1.</h6><p>An <strong>experiment</strong> or <strong>trial</strong> is any procedure, real or hypothetical, that can be infinity repeated and has a well-defined set of possible outcomes, known as the sample space. An <strong>event</strong> is a well-defined set of possible outcomes of the experiment.</p>
<h6 id="Definition-1-2"><a href="#Definition-1-2" class="headerlink" title="Definition 1.2."></a>Definition 1.2.</h6><p>The collection of all possible outcomes of an experiment is called the <strong>sample space</strong> of the experiment.</p>
<p>​    If we think the sample space as a set, the outcomes in the set and events are subsets of the sample space.</p>
<h6 id="Theorem-1-1"><a href="#Theorem-1-1" class="headerlink" title="Theorem 1.1."></a>Theorem 1.1.</h6><p>Let $A, B, C$ be sets. If $A \subset B$ and $B\subset A$, then $A=B$. If $A\subset B$ and $B\subset C$, then $A \subset C$.</p>
<h6 id="Definition-1-3"><a href="#Definition-1-3" class="headerlink" title="Definition 1.3."></a>Definition 1.3.</h6><p>The set that contains no elements is called <strong>empty set</strong> denote by $\varnothing$. </p>
<h6 id="Theorem-1-2"><a href="#Theorem-1-2" class="headerlink" title="Theorem 1.2."></a>Theorem 1.2.</h6><p>If $A$ is a set, then $\varnothing \subset A$.</p>
<h6 id="Definition-1-4"><a href="#Definition-1-4" class="headerlink" title="Definition 1.4."></a>Definition 1.4.</h6><p>A set that contains only finitely many elements are called a <strong>finite set</strong>. Otherwise, it is called an <strong>infinite set</strong>. An infinite set $A$ is called <strong>countable</strong> if there is a one-to-one map between $A$ and $\mathbb{N}$. A set is called <strong>uncountable</strong> if it is neither finite nor countable.</p>
<p>​    <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cardinal_number">More</a> about cardinal number. We don’t talk about it in Probability and Statistics.</p>
<h6 id="Definition-1-5"><a href="#Definition-1-5" class="headerlink" title="Definition 1.5."></a>Definition 1.5.</h6><p>The <strong>complement</strong> of $A$ is the set of all elements that do not belong to $A$, denoted by $A^c$.</p>
<h6 id="Theorem-1-3"><a href="#Theorem-1-3" class="headerlink" title="Theorem 1.3."></a>Theorem 1.3.</h6><p>Let $A$ be a set, then $(A^c)^c = A$. Let $X$ be the whole set, then $X^c = \varnothing$ and $\varnothing^c = X$. </p>
<h6 id="Definition-1-6"><a href="#Definition-1-6" class="headerlink" title="Definition 1.6."></a>Definition 1.6.</h6><p>Let $A, B$ be two sets. The <strong>union</strong> of $A$ and $B$, denoted by $A\cup B$ is defined as </p>
<script type="math/tex; mode=display">
A\cup B = \{x:x\in A\ \text{or}\ x\in B\}</script><p>The <strong>intersection</strong> of $A$ and $B$, denoted by $A \cap B$ is defined as </p>
<script type="math/tex; mode=display">
A\cap B = \{x:x\in A\ \text{and}\ x\in B\}</script><h6 id="Conditions-of-Sample-Spaces-Not-every-set-of-possible-outcomes-will-be-called-an-event"><a href="#Conditions-of-Sample-Spaces-Not-every-set-of-possible-outcomes-will-be-called-an-event" class="headerlink" title="Conditions of Sample Spaces (Not every set of possible outcomes will be called an event)"></a>Conditions of Sample Spaces (Not every set of possible outcomes will be called an event)</h6><ul>
<li>The sample space $\Omega$ must be an event.</li>
<li>If $A \subset \Omega$, then $A^c$ is also an event.</li>
<li>If $A_1,A_2\cdots$ is a countable collection of events, then $\cup_{j=1}^\infty A_j$ is also an event. (In fact, the form of $A_j$ means it must be a countable collection)</li>
</ul>
<h6 id="Definition-1-7"><a href="#Definition-1-7" class="headerlink" title="Definition 1.7."></a>Definition 1.7.</h6><p>Two sets $A$ and $B$ are <strong>disjoint (mutually exclusive)</strong> if $A$ and $B$ have no elements in common, that is if $A\cap B = \varnothing$</p>
<h6 id="Theorem-1-4-De-Morgan’s-Laws"><a href="#Theorem-1-4-De-Morgan’s-Laws" class="headerlink" title="Theorem 1.4 (De Morgan’s Laws)."></a>Theorem 1.4 (De Morgan’s Laws).</h6><p>For every two sets $A$ and $B$ we have </p>
<script type="math/tex; mode=display">
(A\cup B)^c = A^c\cap B^c\hspace{1cm}and\hspace{1cm}(A\cap B)^c = A^c \cup B^c</script><h6 id="Theorem-1-5-Distributive-Properties"><a href="#Theorem-1-5-Distributive-Properties" class="headerlink" title="Theorem 1.5 (Distributive Properties)."></a>Theorem 1.5 (Distributive Properties).</h6><p>For sets $A, B$ and $C$, we have </p>
<script type="math/tex; mode=display">
A\cap(B\cup C) = (A\cap B) \cup (A\cap C)\hspace{1cm}and\hspace{1cm}A\cup(B\cap C) = (A\cup B) \cap (A\cup C)</script><h6 id="Theorem-1-6-Partitioning-a-Set"><a href="#Theorem-1-6-Partitioning-a-Set" class="headerlink" title="Theorem 1.6 (Partitioning a Set)."></a>Theorem 1.6 (Partitioning a Set).</h6><p>For sets $A$ and $B$, we can partition </p>
<script type="math/tex; mode=display">
A = (A\cap B)\cup (A\cap B^c)</script><p>In addition </p>
<script type="math/tex; mode=display">
A \cup B = B \cup (A\cap B^c)</script><hr>
<h4 id="1-2-The-Definition-of-Probability"><a href="#1-2-The-Definition-of-Probability" class="headerlink" title="1.2 The Definition of Probability"></a>1.2 The Definition of Probability</h4><h6 id="Definition-1-8-Axioms-of-Probability"><a href="#Definition-1-8-Axioms-of-Probability" class="headerlink" title="Definition 1.8 (Axioms of Probability)."></a>Definition 1.8 (Axioms of Probability).</h6><p>Let $\Omega$ be a sample space and $A$ be an event. A <strong>probability measure</strong> on $\Omega$ is a map $\mathbf{P}$ from the set all events to the real interval $[0,1]$ that satisfies the following three axioms.</p>
<ol>
<li><p>For every event $A$, <script type="math/tex">\P{A} \geq 0</script>. </p>
</li>
<li><p>For the whole sample space $\Omega$, <script type="math/tex">\P{\Omega}=1</script>. </p>
</li>
<li><p>For every infinite sequence of disjoint events $A_1,A_2\cdots$,</p>
<script type="math/tex; mode=display">
\P{\cup_{j=1}^\infty A_j}=\sum_{j=1}^\infty \P{A_j}</script></li>
</ol>
<h6 id="Theorem-1-7"><a href="#Theorem-1-7" class="headerlink" title="Theorem 1.7."></a>Theorem 1.7.</h6><p>For every finite sequence of $n$ disjoint events  $A_1,A_2\cdots A_n$ </p>
<script type="math/tex; mode=display">
\P{\bigcup_{j=1}^n A_j} = \sum_{j=1}^n \P{A_j}</script><p>$Proof.$ Let $A_j = \varnothing$ for $j&gt;n$, then </p>
<script type="math/tex; mode=display">
\P{\cup_{j=1}^n A_j} = \P{\cup_{j=1}^\infty A_j} = \sum_{j=1}^\infty \P{A_j} = \sum_{j=1}^n \P{A_j}</script><p>Why <script type="math/tex">\P{\varnothing} = 0</script>? Consider a sequence $A_1 = \Omega$ and $A_j = \varnothing$ for $j&gt;1$, which follows </p>
<script type="math/tex; mode=display">
1 = 1+\sum_{j=2}^n \P{\varnothing} \Rightarrow \P{\varnothing} = 0</script><h6 id="Theorem-1-8"><a href="#Theorem-1-8" class="headerlink" title="Theorem 1.8."></a>Theorem 1.8.</h6><p>For every event $A$, <script type="math/tex">\P{A^c} = 1-\P{A}</script>. </p>
<p>$Proof.$ Use Theorem $1.7$ </p>
<h6 id="Theorem-1-9"><a href="#Theorem-1-9" class="headerlink" title="Theorem 1.9."></a>Theorem 1.9.</h6><p>For every event $A$, we have <script type="math/tex">0\leq \P{A}\leq 1</script>.</p>
<p>$Proof.$ Follows the definition of probability and sample space.</p>
<h6 id="Theorem-1-10"><a href="#Theorem-1-10" class="headerlink" title="Theorem 1.10."></a>Theorem 1.10.</h6><p>For events $A$ and $B$, we have </p>
<script type="math/tex; mode=display">
\P{A\cap B^c} = \P{A} -\P{A\cap B}</script><p>$Proof.$ Use partition $A = (A\cap B^c) \cup (A\cap B)$ </p>
<h6 id="Corollary-1"><a href="#Corollary-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>If $A \subset B$, then <script type="math/tex">\P{A} \leq \P{B}</script></p>
<p>$Proof.$ <script type="math/tex">\P{B} = \P{A\cap B^c} + \P{A\cap B} = \P{A}+\P{A\cap B^c} \geq \P{A}</script></p>
<h6 id="Theorem-1-11"><a href="#Theorem-1-11" class="headerlink" title="Theorem 1.11."></a>Theorem 1.11.</h6><p>For events $A$ and $B$, we have <script type="math/tex">\P{A\cup B} = \P{A}+\P{B} - \P{A\cap B}</script></p>
<p>$Proof.$ </p>
<script type="math/tex; mode=display">
\P{A\cup B} = \P{B} + \P{A\cap B^c}\\
\P{A} = \P{A\cap B} + \P{A\cap B^c}</script><h6 id="Corollary-1-1"><a href="#Corollary-1-1" class="headerlink" title="Corollary 1."></a>Corollary 1.</h6><p>For a sequence $A_1,A_2\cdots, A_n$, we have </p>
<script type="math/tex; mode=display">
\P{\bigcup_{i=1}^n A_i} = \sum_{j=1}^n(-1)^{j-1} \sum_{\substack{I_j\subset\{1,\cdots,n\}\\|I_j|=j}} \P{\bigcap_{i\in I_j} A_i}</script><p>$Proof.$ By induction.</p>
<hr>
<h4 id="1-3-Finite-Sample-Spaces"><a href="#1-3-Finite-Sample-Spaces" class="headerlink" title="1.3. Finite Sample Spaces."></a>1.3. Finite Sample Spaces.</h4><h6 id="Definition-1-9"><a href="#Definition-1-9" class="headerlink" title="Definition 1.9."></a>Definition 1.9.</h6><p>A sample space $\Omega$ containing $n$ outcomes $s_1,\cdots, s_n$ is called a <strong>simple sample space</strong> if the probability assigned to each out come is $\frac1n$</p>
<p>​    If $A$ is an event from a simple sample space and $A$ contains $m$ elements, then <script type="math/tex">\P{A} = \frac{m}{n}</script></p>
<hr>
<h4 id="1-4-Counting-Methods"><a href="#1-4-Counting-Methods" class="headerlink" title="1.4. Counting Methods"></a>1.4. Counting Methods</h4><p>​    This part is easy for those who has already known it. I don’t want to waste too much time summarizing this part. Thus I will skip all the theorems or definitions.</p>
<hr>
<h4 id="1-5-Combinatorial-Methods"><a href="#1-5-Combinatorial-Methods" class="headerlink" title="1.5. Combinatorial Methods"></a>1.5. Combinatorial Methods</h4><p>​    The same as $1.4.$ But I will summarize some combinatorial identities.</p>
<h6 id="Definition-1-11"><a href="#Definition-1-11" class="headerlink" title="Definition 1.11."></a>Definition 1.11.</h6><p>Consider a set with $n$ elements. Each subset of size $k$ chosen from this set is called a <strong>combination of $n$ elements</strong> taken $k$ at a time. We denote the number of distinct such combinations by $C_{n,k}$</p>
<h6 id="Definition-1-12"><a href="#Definition-1-12" class="headerlink" title="Definition 1.12."></a>Definition 1.12.</h6><p>The number $C_{n,k}$ is also denoted by the symbol $n\choose k$. That is, for $k=0,1,\cdots,n$, </p>
<script type="math/tex; mode=display">
{n\choose k} = \frac{n!}{k!(n-k)!}</script><h6 id="Combinatorial-identities-1"><a href="#Combinatorial-identities-1" class="headerlink" title="Combinatorial identities 1"></a>Combinatorial identities 1</h6><script type="math/tex; mode=display">
{n\choose k} = {n\choose n-k}</script><h6 id="Combinatorial-identities-2"><a href="#Combinatorial-identities-2" class="headerlink" title="Combinatorial identities 2"></a>Combinatorial identities 2</h6><script type="math/tex; mode=display">
\sum_{k=0}^n {n \choose k} = 2^n\hspace{2cm}\sum_{k=0}^n(-1)^k{n\choose k} = 0</script><h6 id="Combinatorial-identities-3"><a href="#Combinatorial-identities-3" class="headerlink" title="Combinatorial identities 3"></a>Combinatorial identities 3</h6><script type="math/tex; mode=display">
{n\choose k} + {n\choose k+1} = {n+1\choose k+1}</script><h6 id="Combinatorial-identities-4"><a href="#Combinatorial-identities-4" class="headerlink" title="Combinatorial identities 4"></a>Combinatorial identities 4</h6><script type="math/tex; mode=display">
\sum_{k=0}^m {n+k\choose k} = {n+m+1\choose m}</script><h6 id="Combinatorial-identities-5"><a href="#Combinatorial-identities-5" class="headerlink" title="Combinatorial identities 5"></a>Combinatorial identities 5</h6><script type="math/tex; mode=display">
{n+m\choose k} = \sum_{i=0}^k {n\choose i}\cdot {m\choose k-i}</script><h6 id="Combinatorial-identities-6"><a href="#Combinatorial-identities-6" class="headerlink" title="Combinatorial identities 6"></a>Combinatorial identities 6</h6><script type="math/tex; mode=display">
{2n \choose n} = \sum_{i=0}^n {n\choose i}^2</script><hr>
<h4 id="1-6-Multinomial-Coefficients"><a href="#1-6-Multinomial-Coefficients" class="headerlink" title="1.6. Multinomial Coefficients"></a>1.6. Multinomial Coefficients</h4><h6 id="Definition-1-13"><a href="#Definition-1-13" class="headerlink" title="Definition 1.13."></a>Definition 1.13.</h6><p>A <strong>multinomial coefficient</strong> is </p>
<script type="math/tex; mode=display">
{n\choose n_1,n_2,\cdots,n_k} = \frac{n!}{n_1!n_2!\cdots n_k!}</script><p>where $n_1+n_2+\cdots+n_k = n$ </p>
<hr>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>第一章还不是那么抽象, 讲了一些集合的东西以及event的定义. 比较绕的可能就是event是一个定义出的东西而不是任一possible outcomes的set. 接下来定义probability measure是一个从sample space到[0,1]上的一个映射, 以及给出了simple sample space 的定义以及如何计算其中的概率. 个人认为是在概率论中是很具体也很符合直觉的东西了… </p>
<p>这一章里好像最难的是计数的部分, 但我觉得这部分确实没什么好写的, 多做点题就会了…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/25/What'%20this/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewandou">
      <meta itemprop="description" content="快乐摸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yewandou's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/25/What'%20this/" class="post-title-link" itemprop="url">What's this?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-25 23:44:41" itemprop="dateCreated datePublished" datetime="2021-11-25T23:44:41+08:00">2021-11-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-26 00:34:14" itemprop="dateModified" datetime="2021-11-26T00:34:14+08:00">2021-11-26</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>324</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="What’s-this"><a href="#What’s-this" class="headerlink" title="What’s this?"></a>What’s this?</h3><p>一个无聊的憨憨整的博客, 内容大概会是很多的学习经历, 要是有出息的话会放一些自己觉得并不trival的东西上来. 前期内容大概就是一些学习笔记之类的, 由于作者学校CS专业课/大部分数学课都是in English的, 很多东西大概也都是用英语写了.</p>
<p>美观在做了别骂了呜呜呜… 这周一定搞成能看的东西.</p>
<hr>
<h3 id="Who-is-Yewandou"><a href="#Who-is-Yewandou" class="headerlink" title="Who is Yewandou?"></a>Who is Yewandou?</h3><p>大概是作者的网名叭, 不是名人, 高中整过一些物理什么的, 现某校CS大二学生. </p>
<h4 id="主要学习方向："><a href="#主要学习方向：" class="headerlink" title="主要学习方向："></a>主要学习方向：</h4><p>无, 主打一个摸鱼</p>
<h4 id="主要获奖经历："><a href="#主要获奖经历：" class="headerlink" title="主要获奖经历："></a>主要获奖经历：</h4><p>躺了一块ICPC region金牌, 一块ICPC region银牌, 无了.</p>
<h4 id="努力方向："><a href="#努力方向：" class="headerlink" title="努力方向："></a>努力方向：</h4><ol>
<li><p>从变态概率论老师手中活下来 </p>
</li>
<li><p>把手上的ML的入门书看完 </p>
</li>
<li><p>把暑假里看的CS172整理一次笔记.</p>
</li>
<li><p>S12上铂金 (先整个小目标)</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yewandou</p>
  <div class="site-description" itemprop="description">快乐摸鱼</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Yewandou7" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Yewandou7" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yewandou</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">21k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">19 分钟</span>
</div>

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/26/2021 00:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
</div>

        








      </div>
        
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  
  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
